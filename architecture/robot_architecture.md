# Arquitectura del Sistema de Robot DomÃ©stico Interactivo

**VersiÃ³n:** 1.4  
**Fecha:** Febrero 2026  
**Estado:** RevisiÃ³n con ajustes de hardware, Docker Compose, bÃºsqueda de personas, control por voz, memoria de conversaciÃ³n y correcciones de componentes fÃ­sicos

---

## Tabla de Contenidos

1. [Resumen Ejecutivo](#1-resumen-ejecutivo)
2. [Arquitectura General del Sistema](#2-arquitectura-general-del-sistema)
3. [Componente: Backend (Python/FastAPI)](#3-componente-backend-pythonfastapi)
4. [Componente: AplicaciÃ³n Android](#4-componente-aplicaciÃ³n-android)
5. [Componente: ESP32 (Control FÃ­sico)](#5-componente-esp32-control-fÃ­sico)
6. [Protocolos de ComunicaciÃ³n](#6-protocolos-de-comunicaciÃ³n)
7. [GestiÃ³n de Estados del Sistema](#7-gestiÃ³n-de-estados-del-sistema)
8. [Seguridad y Privacidad](#8-seguridad-y-privacidad)
9. [Manejo de Errores y RecuperaciÃ³n](#9-manejo-de-errores-y-recuperaciÃ³n)
10. [Requisitos de Hardware y Software](#10-requisitos-de-hardware-y-software)
11. [Plan de Despliegue](#11-plan-de-despliegue)
12. [MÃ©tricas y Monitoreo](#12-mÃ©tricas-y-monitoreo)

---

## 1. Resumen Ejecutivo

### 1.1 DescripciÃ³n del Proyecto

Sistema robÃ³tico domÃ©stico con capacidades de:
- InteracciÃ³n multimodal (voz, visiÃ³n, texto)
- Reconocimiento de personas y memoria contextual
- Control de movimiento y sensores ambientales
- Interfaz visual expresiva mediante emojis animados
- Procesamiento inteligente mediante LLM

### 1.2 Objetivos Principales

1. **InteracciÃ³n Natural**: ComunicaciÃ³n por voz con wake word y respuestas auditivas
2. **PersonalizaciÃ³n**: Reconocimiento de usuarios y memoria de preferencias
3. **Movilidad AutÃ³noma**: NavegaciÃ³n segura con detecciÃ³n de obstÃ¡culos
4. **Expresividad Visual**: Sistema de emociones mediante OpenMoji
5. **Bajo Costo Operacional**: Gemini Flash Lite (muy econÃ³mico), TTS del sistema Android (sin costo)

### 1.3 Stack TecnolÃ³gico

| Componente | TecnologÃ­a | JustificaciÃ³n |
|------------|-----------|---------------|
| Backend | Python 3.11+ / FastAPI | Ecosistema ML, async, rendimiento |
| Despliegue Backend | Docker Compose | Contenedores locales: FastAPI + Nginx |
| Marco de Agente | LangChain Deep Agents (`deepagents`) | Agente extensible con runtime LangGraph; prepara la arquitectura para MCP/SKILLS/tools en versiones futuras (sin tools actualmente) |
| LLM | Gemini Flash Lite (latest) | Multimodal nativo (audio+imagen+video), muy econÃ³mico, baja latencia, streaming |
| STT | Integrado en Gemini | Gemini recibe audio directamente, sin servicio STT separado |
| TTS | Android TextToSpeech (sistema) | On-device, sin latencia de red, configurable, sin costo |
| Reconocimiento Facial | ML Kit + TFLite FaceNet (solo Android) | On-device, offline, <200ms, sin servidor, solo cÃ¡mara frontal |
| App MÃ³vil | Kotlin / Android 7+ | Soporte dispositivos antiguos, orientaciÃ³n landscape fija |
| Wake Word | Porcupine (Picovoice) | Local, bajo consumo, 3 palabras gratis |
| UI Robot | OpenMoji CDN | Open source, CDN gratis, 4000+ emojis, descarga automÃ¡tica |
| Microcontrolador | ESP32-S3 WROOM (Freenove FNK0082) | WiFi/Bluetooth, GPIO, ESP32-S3, N8R8/N16R8 |
| ComunicaciÃ³n | WebSocket + REST API + Bluetooth LE | Streaming de texto en tiempo real, baja latencia |

---

## 2. Arquitectura General del Sistema

### 2.1 Diagrama de Arquitectura de Alto Nivel

```mermaid
graph TB
    subgraph "Usuario"
        U[Usuario/Familia]
    end
    
    subgraph "Dispositivo Android"
        WW[Wake Word Detector<br/>Porcupine - Hey Robi]
        UI[Interfaz Visual<br/>OpenMoji CDN<br/>Landscape, fondo negro]
        AR[Audio Recorder]
        CAM[CÃ¡mara Frontal<br/>Solo cÃ¡mara delantera]
        FR_ANDROID[Face Recognition<br/>ML Kit + FaceNet TFLite<br/>On-Device, cÃ¡mara frontal]
        BT[Bluetooth Manager]
        TTS_ANDROID[Android TextToSpeech<br/>TTS del Sistema]
        WS_CLIENT[WebSocket Client<br/>Streaming]
        API_CLIENT[REST API Client<br/>GestiÃ³n/Auxiliar]
    end
    
    subgraph "Backend Python/FastAPI + Docker Compose"
        WS_SERVER[WebSocket Server<br/>Streaming<br/>:9393]
        GATEWAY[API Gateway<br/>REST Auxiliar]
        GEMINI[Gemini Flash Lite<br/>Multimodal: audio+imagen+video]
        MEM[Memory Store<br/>SQLite + Historial ConversaciÃ³n]
        EXPR[Expression Manager<br/>EmociÃ³n vÃ­a LLM]
        NGINX[Nginx<br/>Reverse Proxy TLS]
    end
    
    subgraph "ESP32 Microcontrolador"
        BT_ESP[Bluetooth Server]
        MOTOR[Motor Controller<br/>L298N + Gear Motor TT 5V]
        SENS[Sensores<br/>2x HC-SR04 + VL53L0X ToF]
        LED[RGB LED<br/>4 patas, 256 colores]
    end
    
    U -->|Voz| WW
    WW -->|Wake Word OK| CAM
    WW -->|Wake Word OK| AR
    CAM -->|Frames en tiempo real<br/>CÃ¡mara frontal| FR_ANDROID
    FR_ANDROID -->|user_id o unknown| WS_CLIENT
    AR -->|Audio Stream| WS_CLIENT
    
    WS_CLIENT <-->|WebSocket<br/>Streaming bidireccional| NGINX
    NGINX <-->|Proxy| WS_SERVER
    API_CLIENT -->|HTTPS/REST| NGINX
    WS_SERVER --> GEMINI
    GEMINI --> MEM
    GEMINI --> EXPR
    
    WS_SERVER -->|Texto en stream +<br/>emotion tags| WS_CLIENT
    WS_CLIENT --> UI
    WS_CLIENT --> BT
    WS_CLIENT --> TTS_ANDROID
    
    BT <-->|BLE + Heartbeat| BT_ESP
    BT_ESP --> MOTOR
    BT_ESP --> SENS
    BT_ESP --> LED
    SENS -->|TelemetrÃ­a| BT_ESP
    
    UI -->|Display| U
    TTS_ANDROID -->|Voz sintetizada| U
```

### 2.2 Flujo de ActivaciÃ³n: Saludo Inicial (Nuevo Flujo)

Este flujo describe el proceso completo desde que el robot estÃ¡ en reposo hasta que saluda a la persona y queda listo para recibir Ã³rdenes.

**BÃºsqueda de persona:** Al detectar el wake word, el robot inicia automÃ¡ticamente una secuencia de bÃºsqueda: rota hasta 90Â° a la derecha, luego hasta 90Â° a la izquierda, y hace pequeÃ±os movimientos hacia adelante y atrÃ¡s, para encontrar a la persona en el campo de visiÃ³n de la cÃ¡mara frontal. El parÃ¡metro `PERSON_SEARCH_TIMEOUT_MS` (valor inicial: 8000ms = 8 segundos) controla cuÃ¡nto tiempo busca antes de rendirse.

```mermaid
sequenceDiagram
    participant U as Usuario
    participant A as App Android
    participant FR as FaceRecognition (Android)
    participant B as Backend FastAPI
    participant E as ESP32

    Note over A: Estado IDLE (ğŸ¤–)
    U->>A: "Hey Robi"
    A->>A: Wake Word Detectado (Porcupine)
    Note over A: Estado LISTENING (ğŸ‘‚) [Inmediato]

    A->>A: Activar cÃ¡mara frontal en modo bÃºsqueda
    Note over A: Estado SEARCHING (ğŸ”)
    A->>E: BLE: search_rotate(Â±90Â°, speed=30)
    Note over E: Rota derechaâ†’izquierda + mov adelante/atrÃ¡s
    A->>FR: Stream de frames de cÃ¡mara frontal

    loop BÃºsqueda activa (mÃ¡x PERSON_SEARCH_TIMEOUT_MS = 8s)
        FR->>FR: ML Kit detecta rostro en encuadre
        alt Rostro detectado
            FR->>A: Rostro en campo de visiÃ³n
            A->>E: BLE: stop() â€” detener bÃºsqueda
            FR->>FR: Extraer embedding 128D (FaceNet TFLite)
            FR->>FR: Comparar con embeddings en SQLite local
            break Rostro encontrado
                Note over A: Continuar con identificaciÃ³n
            end
        end
    end

    alt Rostro identificado (dentro de 8s)
        alt Usuario reconocido (similitud > 0.7)
            FR->>A: user_id + nombre + confianza
            Note over A: Estado GREETING (ğŸ‘‹)
            A->>B: WS: interaction_start + user_id + imagen
            B-->>A: WS Stream: [emotion:greeting]
            A->>A: Actualiza cara a greeting
            B-->>A: WS Stream: text_chunk "Hola [nombre]!"
            A->>A: Android TTS reproduce saludo personalizado
            B-->>A: WS Stream: stream_end
        else Usuario no reconocido
            Note over A: Estado REGISTERING (â“)
            A->>B: WS: interaction_start + user_id=unknown + imagen
            B-->>A: WS Stream: [emotion:curious]
            B-->>A: WS Stream: text_chunk "Â¿CÃ³mo te llamas?"
            A->>A: Android TTS reproduce pregunta
            B-->>A: WS Stream: stream_end

            Note over A: Estado LISTENING (ğŸ‘‚)
            U->>A: Responde con su nombre (voz)
            A->>A: Graba audio con nombre
            A->>B: WS: Audio con nombre + imagen rostro
            B->>B: Gemini extrae nombre de la respuesta
            B->>B: Guarda usuario en BD
            B-->>A: WS Stream: [emotion:happy]
            B-->>A: WS Stream: text_chunk "Mucho gusto [nombre]!"
            A->>A: Android TTS reproduce saludo de bienvenida
            A->>FR: Guardar embedding en SQLite local
            B-->>A: WS Stream: stream_end
        end
    else Timeout 8s sin rostro detectado
        A->>E: BLE: stop() â€” detener bÃºsqueda
        A->>A: Abortar bÃºsqueda
        Note over A: Estado LISTENING (ğŸ‘‚)
        A->>A: Android TTS: "No puedo verte. Por favor acÃ©rcate al robot"
        Note over A: Esperar a que el usuario se acerque â†’ volver a IDLE
    end

    Note over A: Estado IDLE listo para interacciÃ³n
```

### 2.3 Flujo de InteracciÃ³n General (Post-Saludo)

Una vez completado el saludo inicial, el robot queda en **modo de escucha continua** durante `CONVERSATION_KEEP_ALIVE_MS` (valor inicial: 60 segundos). Durante este perÃ­odo, el usuario puede seguir hablando sin necesidad de repetir el wake word. Solo tras 60 segundos de inactividad, el robot vuelve al estado IDLE completo y requerirÃ¡ el wake word nuevamente.

```mermaid
sequenceDiagram
    participant U as Usuario
    participant A as App Android
    participant B as Backend FastAPI
    participant E as ESP32

    Note over A: Estado IDLE (ğŸ¤–) â€” usuario identificado
    U->>A: "Hey Robi"
    A->>A: Wake Word Detectado
    Note over A: Estado LISTENING (ğŸ‘‚) [Inmediato]

    U->>A: Hace una pregunta u orden (voz)
    A->>A: Graba Audio (hasta silencio 2s o timeout 10s)

    Note over A: Estado THINKING (ğŸ¤”)
    A->>B: WS: audio binario + user_id + contexto sensores

    B->>B: Gemini recibe audio directamente (multimodal)
    B->>B: Gemini razona con memoria e historial del usuario
    B-->>A: WS Stream: [emotion:TAG]
    Note over A: Actualiza cara segÃºn emociÃ³n del LLM

    B-->>A: WS Stream: text_chunk (respuesta en texto en streaming)
    A->>A: Android TTS reproduce texto en tiempo real

    B-->>A: WS Stream: response_meta (emojis + acciones)
    A->>A: Muestra secuencia emojis

    opt AcciÃ³n fÃ­sica requerida (secuencia de comandos)
        A->>E: Comando BLE (move / light) â€” puede ser secuencia
        E-->>A: ConfirmaciÃ³n telemetrÃ­a
    end

    B-->>A: WS Stream: stream_end
    Note over A: Estado LISTENING (ğŸ‘‚) â€” modo escucha continua (60s)
    Note over A: El usuario puede seguir hablando sin wake word
    Note over A: Tras 60s de inactividad â†’ Estado IDLE (ğŸ¤–)
```

### 2.4 Arquitectura de Tres Capas

```mermaid
graph LR
    subgraph "Capa de PresentaciÃ³n"
        A1[App Android]
        A2[Interfaz Visual]
        A3[Audio I/O Streaming]
    end
    
    subgraph "Capa de Negocio"
        B1[WebSocket Server + API Gateway]
        B2[Gemini Flash Lite<br/>Multimodal IA]
        B3[LÃ³gica de Negocio]
        B4[GestiÃ³n de Memoria]
    end
    
    subgraph "Capa de Datos/Actuadores"
        C1[Base de Datos]
        C2[Almacenamiento Archivos]
        C3[ESP32]
        C4[Sensores]
    end
    
    A1 --> B1
    A2 --> B1
    A3 --> B1
    
    B1 --> B2
    B2 --> B3
    B3 --> B4
    
    B3 --> C1
    B3 --> C2
    B1 --> C3
    C4 --> C3
```

---

## 3. Componente: Backend (Python/FastAPI)

### 3.1 Arquitectura del Backend

```mermaid
graph TB
    subgraph "Capa de API"
        MAIN[main.py<br/>FastAPI App]
        WS[websockets/<br/>streaming handler]
        ROUTES[routers/<br/>endpoints REST auxiliares]
        MIDDLEWARE[middleware/<br/>auth, cors, logging]
    end
    
    subgraph "Capa de Servicios"
        AGENT_SERVICE[services/agent.py<br/>LangChain Deep Agent<br/>runtime: LangGraph + historial conversaciÃ³n]
        GEMINI_SERVICE[services/gemini.py<br/>Gemini Flash Lite Multimodal<br/>modelo LLM base]
        EXPR_SERVICE[services/expression.py<br/>EmociÃ³n vÃ­a LLM]
        HISTORY_SERVICE[services/history.py<br/>Historial conversaciÃ³n + compactaciÃ³n]
        MOVEMENT_SERVICE[services/movement.py<br/>EstimaciÃ³n tiempos de movimiento]
    end
    
    subgraph "Capa de Datos"
        MEM_REPO[repositories/memory.py<br/>User Memory CRUD + filtro privacidad]
        USER_REPO[repositories/users.py<br/>User Management]
        MEDIA_REPO[repositories/media.py<br/>File Storage]
    end
    
    subgraph "Capa de Infraestructura"
        DB[(SQLite DB)]
        FILES[(/media/<br/>audio, images)]
    end
    
    MAIN --> WS
    MAIN --> ROUTES
    WS --> MIDDLEWARE
    ROUTES --> MIDDLEWARE
    WS --> AGENT_SERVICE
    WS --> EXPR_SERVICE
    ROUTES --> EXPR_SERVICE

    AGENT_SERVICE --> GEMINI_SERVICE
    AGENT_SERVICE --> HISTORY_SERVICE
    GEMINI_SERVICE --> EXPR_SERVICE
    GEMINI_SERVICE --> MOVEMENT_SERVICE

    AGENT_SERVICE --> MEM_REPO
    GEMINI_SERVICE --> MEM_REPO
    
    MEM_REPO --> DB
    USER_REPO --> DB
    MEDIA_REPO --> FILES
```

### 3.2 Estructura de Directorios

```
backend/
â”œâ”€â”€ docker-compose.yml               # OrquestaciÃ³n de contenedores (FastAPI + Nginx)
â”œâ”€â”€ Dockerfile                       # Imagen Docker para el backend
â”œâ”€â”€ nginx/
â”‚   â”œâ”€â”€ nginx.conf                   # ConfiguraciÃ³n Nginx reverse proxy TLS
â”‚   â””â”€â”€ certs/
â”‚       â”œâ”€â”€ cert.pem                 # Certificado TLS autofirmado
â”‚       â””â”€â”€ key.pem                  # Clave privada TLS
â”œâ”€â”€ main.py                      # Punto de entrada FastAPI + WebSocket
â”œâ”€â”€ config.py                    # ConfiguraciÃ³n centralizada
â”œâ”€â”€ requirements.txt             # Dependencias Python
â”œâ”€â”€ websockets/
â”‚   â”œâ”€â”€ streaming.py            # WebSocket handler principal (interacciÃ³n de voz)
â”‚   â”œâ”€â”€ protocol.py             # Protocolo de mensajes WebSocket
â”‚   â””â”€â”€ auth.py                 # AutenticaciÃ³n WebSocket (API Key en handshake)
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ users.py                # GestiÃ³n de usuarios (REST)
â”‚   â”œâ”€â”€ memory.py               # GestiÃ³n de memoria (REST)
â”‚   â””â”€â”€ health.py               # Health checks (REST)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ agent.py                # LangChain Deep Agent (deepagents) â€” orquesta la conversaciÃ³n
â”‚   â”‚                           #   Modelo: Gemini Flash Lite (via langchain-google-genai)
â”‚   â”‚                           #   Historial de conversaciÃ³n con compactaciÃ³n cada 20 mensajes
â”‚   â”‚                           #   Sin tools actualmente; extensible con MCP/SKILLS/tools
â”‚   â”œâ”€â”€ gemini.py               # InicializaciÃ³n y configuraciÃ³n del modelo Gemini
â”‚   â”‚                           #   (ChatGoogleGenerativeAI) â€” utilizado por agent.py
â”‚   â”œâ”€â”€ history.py              # GestiÃ³n del historial de conversaciÃ³n
â”‚   â”‚                           #   CompactaciÃ³n: resumen msgs 1-15 cada 20 mensajes
â”‚   â”‚                           #   Filtro de privacidad (no guarda info sensible)
â”‚   â”‚                           #   Operaciones completamente asÃ­ncronas
â”‚   â”œâ”€â”€ movement.py             # EstimaciÃ³n de tiempos de movimiento del robot
â”‚   â”‚                           #   Calcula duraciÃ³n de secuencias para sincronizar emojis
â”‚   â”œâ”€â”€ expression.py           # Parser de emotion tags del LLM
â”‚   â””â”€â”€ intent.py               # Clasificador de intenciones (incl. captura, movimiento)
â”œâ”€â”€ repositories/
â”‚   â”œâ”€â”€ memory.py               # CRUD memoria de usuarios + filtro de privacidad
â”‚   â”œâ”€â”€ users.py                # CRUD usuarios
â”‚   â””â”€â”€ media.py                # GestiÃ³n de archivos
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ requests.py             # Modelos Pydantic request
â”‚   â”œâ”€â”€ responses.py            # Modelos Pydantic response
â”‚   â”œâ”€â”€ ws_messages.py          # Modelos de mensajes WebSocket
â”‚   â””â”€â”€ entities.py             # Entidades de dominio
â”œâ”€â”€ middleware/
â”‚   â”œâ”€â”€ auth.py                 # API Key authentication (REST + WS)
â”‚   â”œâ”€â”€ error_handler.py        # Manejo global de errores
â”‚   â””â”€â”€ logging.py              # Logging estructurado
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ audio.py                # Procesamiento de audio (para Gemini)
â”‚   â”œâ”€â”€ image.py                # Procesamiento de imÃ¡genes
â”‚   â””â”€â”€ validators.py           # Validaciones personalizadas
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                   # Pruebas unitarias
â”‚   â”œâ”€â”€ integration/            # Pruebas de integraciÃ³n
â”‚   â””â”€â”€ streamlit_simulator/
â”‚       â””â”€â”€ app.py              # Simulador Android en Streamlit (REST + WebSocket)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ robot.db                # SQLite database
â””â”€â”€ media/
    â”œâ”€â”€ uploads/                # Archivos subidos temporalmente (audio, imagen, video)
    â””â”€â”€ logs/                   # Logs de sistema
```

### 3.3 Canal Principal: WebSocket `/ws/interact`

El canal principal de interacciÃ³n utiliza WebSocket para streaming bidireccional en tiempo real. Esto elimina el "dead air" (silencio muerto) que se producÃ­a con el modelo REST request/response, permitiendo que el robot comience a hablar mientras el backend aÃºn genera el resto de la respuesta.

#### ConexiÃ³n WebSocket

```
URL: wss://192.168.2.200:9393/ws/interact
AutenticaciÃ³n: API Key enviada en handshake inicial
Protocolo: JSON (mensajes de control) + Binary (audio del usuario)
Keepalive: Ping/Pong cada 30s
Servidor: 192.168.2.200 (IP fija del servidor local)
Puerto: 9393
```

#### Mensajes del Cliente (Android â†’ Backend)

```json
// 1. Handshake inicial (primer mensaje despuÃ©s de conectar)
{
  "type": "auth",
  "api_key": "<secret-key>",
  "device_id": "android-uuid"
}

// 2. Inicio de interacciÃ³n
{
  "type": "interaction_start",
  "request_id": "uuid-v4",
  "user_id": "user_juan_123",   // "unknown" si face recognition no identificÃ³ a la persona
  "face_recognized": true,       // false cuando user_id = "unknown"
  "face_confidence": 0.87,       // score de similitud coseno (0-1), null si no reconocido
  "context": {
    "location": "sala",
    "battery_level": 75,
    "sensors": {}
  }
}

// 3. Audio (binario): Frames de audio enviados como binary messages
//    Formato: AAC/Opus, 16kHz, mono

// 4. Imagen de registro o contexto visual
{
  "type": "image",
  "request_id": "uuid-v4",
  "purpose": "registration",    // "registration" | "context"
  "data": "<base64-encoded-jpeg>"
}

// 4b. Video de contexto (cuando el usuario pide grabar un video)
{
  "type": "video",
  "request_id": "uuid-v4",
  "duration_ms": 10000,          // DuraciÃ³n del video capturado
  "data": "<base64-encoded-mp4>" // Video comprimido (max 20MB)
}

// 5. Fin de audio
{
  "type": "audio_end",
  "request_id": "uuid-v4"
}

// 6. Texto directo (alternativa a audio)
{
  "type": "text",
  "request_id": "uuid-v4",
  "content": "Â¿QuÃ© hora es?"
}
```

#### Mensajes del Servidor (Backend â†’ Android) â€” Streaming

```json
// 1. ConfirmaciÃ³n de autenticaciÃ³n
{
  "type": "auth_ok",
  "session_id": "uuid-v4"
}

// 1b. ConfirmaciÃ³n de registro de nuevo usuario
//     Enviado tras guardar el usuario en BD (flujo REGISTERING)
{
  "type": "user_registered",
  "user_id": "user_maria_a3f2c1",
  "name": "MarÃ­a"
}

// 2. Emotion tag (enviado ANTES del texto, para actualizar cara inmediatamente)
{
  "type": "emotion",
  "request_id": "uuid-v4",
  "emotion": "empathy",
  "user_identified": "user_juan_123",
  "confidence": 0.95
}

// 3. Fragmento de texto de respuesta (streaming progresivo desde Gemini)
//    Android TextToSpeech consume estos chunks a medida que llegan,
//    sin esperar el texto completo. La sÃ­ntesis de voz ocurre on-device.
{
  "type": "text_chunk",
  "request_id": "uuid-v4",
  "text": "Hola Juan, cÃ³mo estÃ¡s"
}

// 4. Metadata de respuesta (enviado al finalizar el stream de texto)
//    Incluye secuencias de movimiento con duraciÃ³n calculada para sincronizar emojis
{
  "type": "response_meta",
  "request_id": "uuid-v4",
  "response_text": "Hola Juan, Â¿cÃ³mo estÃ¡s?",
  "expression": {
    "emojis": ["1F44B", "1F603", "2728"],
    "duration_per_emoji": 2000,
    "transition": "bounce"
  },
  "actions": [
    // AcciÃ³n Ãºnica:
    {
      "type": "move",
      "params": {
        "direction": "forward",
        "speed": 50,
        "duration_ms": 2000
      }
    },
    // Secuencia de movimientos (ej: â€œrota 3 veces a la derechaâ€):
    // El backend calcula duration_ms de cada paso para sincronizar el emoji
    {
      "type": "move_sequence",
      "total_duration_ms": 15000,   // Tiempo total de toda la secuencia
      "emotion_during": "happy",   // Emoji a mostrar durante toda la secuencia
      "steps": [
        { "direction": "right", "speed": 40, "duration_ms": 5000 },
        { "direction": "right", "speed": 40, "duration_ms": 5000 },
        { "direction": "right", "speed": 40, "duration_ms": 5000 }
      ]
    },
    {
      "type": "light",
      "params": {
        "color": "rgb(0,100,255)",
        "intensity": 80
      }
    }
  ]
}

// 5. Fin de stream
{
  "type": "stream_end",
  "request_id": "uuid-v4",
  "processing_time_ms": 850
}

// 6. Error
{
  "type": "error",
  "request_id": "uuid-v4",
  "error_code": "GEMINI_TIMEOUT",
  "message": "El servicio de procesamiento no estÃ¡ disponible",
  "recoverable": true
}
```

#### Ventajas del Modelo Streaming

| Aspecto | REST (v1.0) | WebSocket Streaming (v1.3) |
|---------|-------------|---------------------------|
| Latencia percibida | 3-5s (espera completa) | <800ms (primer text_chunk) |
| Dead air | SÃ­, durante todo el procesamiento | MÃ­nimo, la emociÃ³n se muestra de inmediato |
| Entrega de respuesta | Texto completo despuÃ©s de generar | text_chunks progresivos durante generaciÃ³n Gemini |
| SÃ­ntesis de voz | Backend separado (STT+LLM+TTS) | Gemini Flash Lite multimodal + Android TTS on-device |
| SincronizaciÃ³n cara | DespuÃ©s de recibir respuesta completa | Inmediata vÃ­a emotion tag |
| Experiencia usuario | Robot parece congelado | Robot parece vivo y responsivo |


### 3.4 Flujo de Procesamiento Interno (Streaming)

```mermaid
flowchart TD
    START[WebSocket: Mensaje recibido] --> AUTH{Â¿SesiÃ³n autenticada?}
    AUTH -->|No| ERROR_AUTH[Enviar error auth + cerrar WS]
    AUTH -->|SÃ­| TYPE{Tipo de mensaje}
    
    TYPE -->|audio_binary| BUFFER[Acumular audio en buffer]
    TYPE -->|audio_end| GEMINI[Enviar audio a Gemini Flash Lite]
    TYPE -->|image| GEMINI
    TYPE -->|video| GEMINI
    TYPE -->|text| GEMINI
    
    BUFFER --> TYPE
    GEMINI --> USER_ID[Identificar Usuario en contexto]
    
    USER_ID --> LOAD_MEM[Cargar Memoria Usuario]
    GEMINI --> LOAD_MEM
    
    LOAD_MEM --> LLM[Procesar con Gemini Flash Lite<br/>Prompt incluye instrucciÃ³n<br/>de emotion tag]
    
    LLM --> PARSE_EMOTION[Parsear emotion tag<br/>del output de Gemini]
    PARSE_EMOTION --> STREAM_EMOTION[WS Stream: Enviar emotion tag<br/>al cliente INMEDIATAMENTE]
    
    LLM --> TEXT_STREAM[Generar texto en chunks desde Gemini]
    TEXT_STREAM --> STREAM_TEXT[WS Stream: Enviar text_chunk<br/>progresivamente]
    
    LLM --> ACTIONS[Determinar Acciones]
    LLM --> EXPR[Seleccionar secuencia emojis]
    
    ACTIONS --> STREAM_META[WS Stream: Enviar metadata<br/>emojis + acciones]
    EXPR --> STREAM_META
    
    STREAM_TEXT --> SAVE_MEM[Guardar en Memoria]
    STREAM_META --> STREAM_END[WS Stream: stream_end]
    SAVE_MEM --> STREAM_END
    
    ERROR_AUTH --> END[Fin]
    STREAM_END --> END
```


### 3.5 Agente de IA: LangChain Deep Agents

El backend utiliza **LangChain Deep Agents** (`deepagents`) como *agent harness* para orquestar la interacciÃ³n con Gemini Flash Lite. Esta decisiÃ³n arquitectÃ³nica separa intencionalmente el **modelo de IA** del **agente que lo controla**, dejando la puerta abierta para extender las capacidades del robot sin modificar la arquitectura base.

#### Estado Actual y EvoluciÃ³n Planificada

En la versiÃ³n actual (v1.4) el agente opera **sin herramientas adicionales**: recibe el input del usuario (audio, imagen o video), lo procesa con Gemini Flash Lite y devuelve la respuesta en streaming. Las siguientes extensiones estÃ¡n diseÃ±adas en la arquitectura pero **no estÃ¡n implementadas**:

| ExtensiÃ³n | DescripciÃ³n | Estado |
|-----------|-------------|--------|
| Tools | Funciones Python invocables por el agente (ej: consultar clima, leer calendario) | Planificado |
| Skills | Capacidades especializadas del robot (ej: contar chistes, recitar recetas) | Planificado |
| MCP | Model Context Protocol â€” acceso estandarizado a servicios externos | Planificado |
| Subagentes | DelegaciÃ³n de subtareas a agentes especializados (vÃ­a tool `task` de deepagents) | Planificado |

> **Nota de diseÃ±o:** La ausencia de tools en v1.3 es intencional. El objetivo es validar el flujo base (audio â†’ Gemini â†’ text\_chunks â†’ Android TTS) antes de aÃ±adir complejidad de herramientas.

#### Modelo de ImplementaciÃ³n

```python
# services/agent.py (simplificado)
from deepagents import create_deep_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from .config import settings
from .prompts import SYSTEM_PROMPT  # Prompt TTS-safe (ver secciÃ³n 3.7)

# Modelo base: Gemini Flash Lite
model = ChatGoogleGenerativeAI(
    model=settings.GEMINI_MODEL,        # "gemini-2.0-flash-lite"
    google_api_key=settings.GEMINI_API_KEY,
    streaming=True,
)

# Agente sin tools actualmente
# Para aÃ±adir tools en el futuro: tools=[get_weather, control_lights, ...]
# Para MCP: tools=load_mcp_tools("npx", ["-y", "@modelcontextprotocol/server-xxx"])
agent = create_deep_agent(
    model=model,
    tools=[],   # VacÃ­o en v1.3 â€” se poblarÃ¡ en versiones futuras
    system_prompt=SYSTEM_PROMPT,
)
```

#### Runtime: LangGraph

El agente usa **LangGraph** como runtime (incluido en `deepagents`), lo que aporta de forma gratuita:

- **Streaming nativo**: compatible con el protocolo `text_chunk` ya implementado en WebSocket
- **Persistencia de estado**: base para memoria de largo plazo entre conversaciones
- **Human-in-the-loop**: capacidad de pausar y esperar input adicional del usuario
- **Durabilidad**: reanudaciÃ³n de agentes interrumpidos por fallos de red

---

### 3.6 GestiÃ³n de Memoria de Usuario

#### Modelo de Datos

```
Tabla: users
- id: INTEGER PRIMARY KEY
- user_id: VARCHAR(50) UNIQUE (ej: "user_juan_123")
- name: VARCHAR(100)
- face_embedding: BLOB (vector 128D â€” sincronizado desde Android)
- preferences: JSON
- created_at: TIMESTAMP
- last_seen: TIMESTAMP

Tabla: memories
- id: INTEGER PRIMARY KEY
- user_id: VARCHAR(50) FK
- memory_type: VARCHAR(20) (fact, preference, conversation)
- content: TEXT
- importance: INTEGER (1-10)
- timestamp: TIMESTAMP
- expires_at: TIMESTAMP (nullable)

Tabla: interactions
- id: INTEGER PRIMARY KEY
- user_id: VARCHAR(50) FK
- request_type: VARCHAR(20) (audio, vision, text)
- summary: TEXT
- timestamp: TIMESTAMP

Tabla: conversation_history (historial en memoria, sesiÃ³n activa)
- id: INTEGER PRIMARY KEY
- session_id: VARCHAR(50) FK
- role: VARCHAR(10) (user | assistant)
- content: TEXT
- message_index: INTEGER
- timestamp: TIMESTAMP
- is_compacted: BOOLEAN  -- indica si forma parte de un resumen compactado
```

#### Historial de ConversaciÃ³n y CompactaciÃ³n

El historial de conversaciÃ³n se mantiene en memoria durante la sesiÃ³n activa para garantizar coherencia. Para evitar que la ventana de contexto del LLM se llene, se aplica una estrategia de **compactaciÃ³n automÃ¡tica asÃ­ncrona** cada 20 mensajes:

```
Estrategia de compactaciÃ³n:
  Disparador: Al llegar al mensaje nÃºmero 20 de la sesiÃ³n
  Proceso asÃ­ncrono (no bloquea la interacciÃ³n):
    1. Tomar mensajes 1-15 del historial
    2. Generar un resumen compactado con Gemini Flash Lite
    3. Reemplazar los 15 mensajes por el Ãºnico resumen
    4. Mantener los mensajes 16-20 intactos (mÃ¡s recientes)
  Resultado: El contexto siempre tendrÃ¡:
    - 1 mensaje de resumen (equivale a msgs 1-15)
    - N mensajes recientes (â‰¤ 5 al momento de compactar)

ImplementaciÃ³n:
  - Verificar si LangChain/LangGraph admite compactaciÃ³n de memoria nativamente
    (ConversationSummaryBufferMemory o similar)
  - Si no estÃ¡ disponible nativamente â†’ implementar manualmente en services/history.py
  - Toda operaciÃ³n de escritura/compactaciÃ³n es asÃ­ncrona (asyncio.create_task)
  - El robot responde normalmente mientras la compactaciÃ³n ocurre en background
```

#### Filtro de Privacidad (Memoria Persistente)

La memoria persistente (guardada en BD para futuras sesiones) aplica un filtro obligatorio antes de guardar cualquier dato:

```
PERMITIDO guardar:
  - Nombre del usuario âœ…
  - Gustos y preferencias (comida, mÃºsica, hobbies) âœ…
  - Recordatorios no sensibles âœ…
  - Contexto conversacional general âœ…

NUNCA guardar (filtrado automÃ¡ticamente):
  - ContraseÃ±as o PINs âŒ
  - NÃºmeros de tarjeta o informaciÃ³n bancaria âŒ
  - Documentos de identidad (DNI, pasaporte, etc.) âŒ
  - InformaciÃ³n mÃ©dica sensible âŒ
  - Cualquier dato que el filtro clasifique como PII crÃ­tico âŒ

ImplementaciÃ³n: repositories/memory.py aplica el filtro en todos
los mÃ©todos de escritura. El filtro usa Gemini para clasificar el
contenido antes de persistir. OperaciÃ³n 100% asÃ­ncrona.
```

#### Estrategia de RecuperaciÃ³n de Memoria

```mermaid
graph LR
    A[Nueva InteracciÃ³n] --> B{Â¿Usuario identificado?}
    B -->|SÃ­| C[Recuperar memoria relevante]
    B -->|No| D[Memoria genÃ©rica/ninguna]
    
    C --> E[Filtrar por importancia > 5]
    E --> F[Ordenar por timestamp DESC]
    F --> G[Top 5 memorias recientes]
    
    G --> H[Inyectar en contexto LLM]
    D --> H
    
    C --> HIST[Cargar historial sesiÃ³n activa]
    HIST --> H
    
    H --> I[LLM genera respuesta contextual]
    I --> J[Extraer nuevas memorias en background]
    J --> K[Filtrar privacidad]
    K --> L[Guardar si importancia > 3]
    L --> M[Verificar compactaciÃ³n si > 20 msgs]
```

### 3.7 Sistema de Emociones Dirigidas por LLM

#### Estrategia: Emotion Tags en el Output Stream del LLM

El sistema de emociones estÃ¡ **completamente dirigido por el LLM**, eliminando el sistema de reglas bÃ¡sicas por palabras clave que causaba disonancia cognitiva (ej: mostrar emoji de sol cuando el usuario dice "estoy terriblemente acalorado" hablando de fiebre). El LLM comprende el contexto semÃ¡ntico y genera una etiqueta de emociÃ³n coherente con su respuesta.

**Â¿Por quÃ© no usar reglas por palabras clave?**
Un mapeo simple de palabras a emojis no tiene concepto de contexto. Si un usuario dice "estoy terriblemente caliente, creo que tengo fiebre", un sistema de reglas mapea "caliente" a un emoji de sol (â˜€ï¸) o lentes de sol (ğŸ˜), mientras el LLM genera una respuesta empÃ¡tica como "Lo siento mucho, espero que te mejores". El resultado: la cara del robot contradice sus palabras, generando una experiencia rota o incluso sociopÃ¡tica.

**ImplementaciÃ³n:**

El prompt del LLM incluye una instrucciÃ³n para clasificar el sentimiento de su respuesta con un tag al inicio del output stream:

```
InstrucciÃ³n en System Prompt:
"Antes de tu respuesta, emite una etiqueta de emociÃ³n que refleje 
el sentimiento de TU respuesta (no del usuario). 
Formato: [emotion:TAG]
Tags vÃ¡lidos: happy, excited, sad, empathy, confused, surprised, 
love, cool, greeting, neutral, curious, worried, playful
Ejemplo: [emotion:empathy] Lo siento mucho, espero que te mejores pronto."
```

**System Prompt completo del modelo:**

```
Eres Robi, un robot domÃ©stico amigable e interactivo. Tienes memoria de las personas
con las que interactuÃ³as y adaptas tus respuestas segÃºn el contexto y las preferencias
de cada usuario.

INSTRUCCIONES DE EMOCIÃ“N:
Antes de cada respuesta, emite una etiqueta de emociÃ³n que refleje el sentimiento
de TU respuesta (no el del usuario). Formato: [emotion:TAG]
Tags vÃ¡lidos: happy, excited, sad, empathy, confused, surprised, love, cool,
greeting, neutral, curious, worried, playful
Ejemplo: [emotion:empathy] Lo siento mucho, espero que te mejores pronto.

INSTRUCCIONES DE RESPUESTA (OBLIGATORIO):
- Da respuestas cortas de mÃ¡ximo un pÃ¡rrafo, a menos que el usuario pida
  explÃ­citamente una respuesta completa y detallada.
- Tus respuestas serÃ¡n leÃ­das en voz alta por un sistema Text-to-Speech.
  Por eso es CRUCIAL seguir estas reglas:
  * Escribe los nÃºmeros completamente en palabras: "quinientos" en lugar de "500",
    "tres mil" en lugar de "3.000" o "3,000".
  * Escribe los sÃ­mbolos como palabras: "mÃ¡s" en lugar de "+", "por ciento"
    en lugar de "%", "euros" en lugar de "â‚¬".
  * No uses fÃ³rmulas matemÃ¡ticas, tablas, listas con viÃ±etas, asteriscos,
    guiones decorativos, separadores de miles ni ninguna notaciÃ³n que suene
    extraÃ±o al ser leÃ­da linealmente.
  * Redacta en prosa fluida y natural, como si hablaras directamente con alguien.
  * Si necesitas enumerar elementos, hÃ¡zlo con "primero", "segundo", "y por Ãºltimo"
    en lugar de "1.", "2.", "3.".
  * Evita acrÃ³nimos poco comunes sin explicarlos. Pronuncia las siglas como
    palabras o explÃ­calas: "la Inteligencia Artificial" en vez de solo "la IA".
- Habla siempre en el idioma que usa el usuario.
```

**Flujo de procesamiento:**

```mermaid
flowchart TD
    A[Gemini genera output stream] --> B[Primer token: emotion tag]
    B --> C[Parser extrae tag de emociÃ³n]
    C --> D[Enviar emotion tag vÃ­a WebSocket<br/>ANTES del texto]
    D --> E[Android actualiza cara del robot<br/>INMEDIATAMENTE]
    E --> F[Gemini continÃºa generando texto]
    F --> G[text_chunks enviados vÃ­a WebSocket]
    G --> H[Android TTS reproduce texto en streaming]
```

**Costo:** Un token adicional por interacciÃ³n (despreciable). La emociÃ³n se sincroniza perfectamente con la intenciÃ³n del LLM, ya que ambos provienen de la misma fuente.

#### Mapeo de Emotion Tags a Emojis

```
Emotion Tags â†’ Emojis:
- happy: 1F600, 1F603, 1F604, 1F60A (selecciÃ³n aleatoria del grupo)
- excited: 1F929, 1F389, 1F38A, 2728
- sad: 1F622, 1F625, 1F62D
- empathy: 1F97A, 1F615, 2764-FE0F
- confused: 1F615, 1F914, 2753
- surprised: 1F632, 1F62E, 1F92F
- love: 2764-FE0F, 1F60D, 1F970, 1F498
- cool: 1F60E, 1F44D, 1F525
- greeting: 1F44B, 1F917
- neutral: 1F642, 1F916
- curious: 1F9D0, 1F50D
- worried: 1F61F, 1F628
- playful: 1F61C, 1F609, 1F638
```

#### Estados Fijos (No dependen del LLM)

```
Estos estados se controlan directamente por la mÃ¡quina de estados,
sin pasar por el LLM:

- IDLE: 1F916 (ğŸ¤–)
- LISTENING: 1F442 (ğŸ‘‚)
- THINKING: 1F914 (ğŸ¤”)
- ERROR: 1F615 (ğŸ˜•)
- DISCONNECTED: 1F50C (ğŸ”Œ)
```

#### Emojis Contextuales Adicionales

```
La metadata de respuesta puede incluir emojis contextuales 
adicionales seleccionados por el LLM en su respuesta:

Contextos (200+ emojis):
- weather: 2600-FE0F, 1F327, 26C5, 2744-FE0F
- food: 1F354, 1F355, 1F371, 1F382
- music: 1F3B5, 1F3B6, 1F3A4
- sports: 26BD, 1F3C0, 1F3BE
- tech: 1F4BB, 1F4F1, 1F916
- time: 23F0, 1F551, 231B
- location: 1F4CD, 1F5FA, 1F30D
```

### 3.8 ConfiguraciÃ³n y Variables de Entorno

```
# Backend .env file

# Servidor
HOST=0.0.0.0
PORT=9393
ENVIRONMENT=production
SERVER_IP=192.168.2.200

# TLS (gestionado por Nginx en Docker Compose)
# Los certificados van en nginx/certs/

# WebSocket
WS_PING_INTERVAL=30
WS_PING_TIMEOUT=10
WS_MAX_MESSAGE_SIZE_MB=50

# Seguridad
API_KEY=<generar-clave-aleatoria-32-chars>
ALLOWED_ORIGINS=https://192.168.2.200  # IP del servidor local

# LLM (Gemini)
GEMINI_API_KEY=<tu-api-key-google-ai-studio>
GEMINI_MODEL=gemini-2.0-flash-lite  # Modelo principal
GEMINI_MAX_OUTPUT_TOKENS=512
GEMINI_TEMPERATURE=0.7

# ConversaciÃ³n
CONVERSATION_KEEP_ALIVE_MS=60000      # 60 segundos de escucha continua tras interacciÃ³n
CONVERSATION_COMPACTION_THRESHOLD=20  # Compactar cada 20 mensajes (resumen msgs 1-15)

# BÃºsqueda de persona
PERSON_SEARCH_TIMEOUT_MS=8000         # 8 segundos mÃ¡ximo para buscar persona tras wake word

# Base de Datos
DATABASE_URL=sqlite:///./data/robot.db

# Almacenamiento
MEDIA_DIR=./media
MAX_UPLOAD_SIZE_MB=50

# Logging
LOG_LEVEL=INFO
LOG_FILE=./media/logs/robot.log
```

### 3.9 Manejo de Errores del Backend

```mermaid
flowchart TD
    START[Error Ocurre] --> CATCH[Exception Caught]
    
    CATCH --> TYPE{Tipo de Error}
    
    TYPE -->|Validation Error| V[422 Unprocessable Entity]
    TYPE -->|Auth Error| A[401 Unauthorized]
    TYPE -->|Not Found| N[404 Not Found]
    TYPE -->|External Service| E[503 Service Unavailable]
    TYPE -->|Internal| I[500 Internal Server Error]
    
    V --> LOG[Log con nivel WARNING]
    A --> LOG
    N --> LOG
    E --> LOG2[Log con nivel ERROR]
    I --> LOG2
    
    LOG --> RESPONSE[Response con error_code]
    LOG2 --> RESPONSE
    
    RESPONSE --> CLIENT[Cliente recibe JSON estructurado]
    
    CLIENT --> RETRY{Â¿Error recuperable?}
    RETRY -->|SÃ­ 503| QUEUE[Reintentar con backoff]
    RETRY -->|No| SHOW[Mostrar error al usuario]
```

**Formato de Respuesta de Error:**

```json
{
  "error": true,
  "error_code": "GEMINI_TIMEOUT",
  "message": "El servicio de procesamiento de lenguaje no estÃ¡ disponible",
  "details": "Gemini API timeout after 30s",
  "recoverable": true,
  "retry_after": 5,
  "timestamp": "2026-02-08T10:30:00Z"
}
```

---

## 4. Componente: AplicaciÃ³n Android

### 4.1 Arquitectura de la AplicaciÃ³n

```mermaid
graph TB
    subgraph "UI Layer"
        ACT[MainActivity]
        FACE[RobotFaceView]
        NOTIF[NotificationManager]
    end
    
    subgraph "Service Layer"
        SVC[RobotService<br/>Foreground Service]
        WATCHDOG[ServiceWatchdog<br/>AlarmManager]
        WW[WakeWordDetector]
        AUDIO_REC[AudioRecorder]
        TTS_MGR[TtsManager<br/>Android TextToSpeech]
        CAM_MGR[CameraManager<br/>Foto/Video + Reconocimiento]
    end
    
    subgraph "Data Layer"
        WS[WebSocketClient<br/>OkHttp WebSocket]
        API[RobotApiClient<br/>Retrofit REST]
        BT[BluetoothManager<br/>+ HeartbeatSender]
        PREFS[EncryptedSharedPreferences]
        CACHE[EmojiCache]
        CERT[CertificatePinner<br/>TLS Pinning]
    end
    
    subgraph "Domain Layer"
        STATE[StateManager]
        EXPR_MGR[ExpressionManager<br/>Emotion Tag Parser]
    end
    
    ACT --> FACE
    ACT --> SVC
    SVC --> WW
    SVC --> AUDIO_REC
    SVC --> AUDIO_PLAY
    SVC --> CAM_MGR
    WATCHDOG -.->|Supervisa| SVC
    
    WW --> STATE
    STATE --> FACE
    STATE --> AUDIO_REC
    
    AUDIO_REC --> WS
    CAM_MGR --> WS
    WS --> STATE
    WS --> TTS_MGR
    WS --> CERT
    API --> CERT
    
    STATE --> EXPR_MGR
    EXPR_MGR --> FACE
    EXPR_MGR --> CACHE
    
    WS --> BT
    BT --> STATE
    
    SVC --> PREFS
    TTS_MGR --> PREFS
```

### 4.2 Estructura de Directorios

```
android-app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ manifests/
â”‚   â”‚   â””â”€â”€ AndroidManifest.xml
â”‚   â”œâ”€â”€ java/com/robot/
â”‚   â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”‚   â”œâ”€â”€ MainActivity.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ RobotFaceView.kt
â”‚   â”‚   â”‚   â””â”€â”€ PermissionsActivity.kt
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ RobotService.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ ServiceWatchdog.kt          # Watchdog externo vÃ­a AlarmManager
â”‚   â”‚   â”‚   â”œâ”€â”€ WakeWordDetector.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ AudioRecorder.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ TtsManager.kt               # Android TextToSpeech: configura voz, velocidad, tono
â”‚   â”‚   â”‚   â”œâ”€â”€ CameraManager.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ PhotoVideoCaptureService.kt  # Captura foto/video cuando el usuario lo solicita
â”‚   â”‚   â”‚   â””â”€â”€ FaceSearchService.kt        # Orquesta bÃºsqueda facial tras wake word
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ websocket/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RobotWebSocketClient.kt  # Cliente WebSocket principal
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ WsMessageParser.kt       # Parser de mensajes WS (incl. text_chunk, emotion)
â”‚   â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RobotApiClient.kt        # REST para endpoints auxiliares
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ApiService.kt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ AuthInterceptor.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ bluetooth/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ BluetoothManager.kt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ESP32Protocol.kt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ HeartbeatSender.kt       # EnvÃ­o de heartbeat cada 1s
â”‚   â”‚   â”‚   â”œâ”€â”€ facerecognition/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FaceRecognitionManager.kt  # Orquesta detecciÃ³n + reconocimiento
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FaceNetModel.kt            # TFLite FaceNet: genera embeddings 128D
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FaceDetector.kt            # ML Kit: detecta bounding boxes
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FaceEmbeddingStore.kt      # CRUD embeddings en SQLite local
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ FaceSimilarityEngine.kt    # Cosine similarity + bÃºsqueda KNN
â”‚   â”‚   â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ EmojiCache.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ security/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ CertificatePinning.kt    # Certificate pinning config
â”‚   â”‚   â”‚   â””â”€â”€ preferences/
â”‚   â”‚   â”‚       â””â”€â”€ AppPreferences.kt
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RobotState.kt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Expression.kt
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ EmotionTag.kt                # Modelo de emotion tags
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ FaceMatch.kt                 # Resultado del reconocimiento facial
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CaptureResult.kt             # Resultado de captura foto/video
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RobotResponse.kt
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ESP32Command.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ StateManager.kt
â”‚   â”‚   â”‚   â”œâ”€â”€ ExpressionManager.kt         # Parsea emotion tags del LLM + sincroniza con TTS
â”‚   â”‚   â”‚   â””â”€â”€ GreetingOrchestrator.kt      # LÃ³gica: saludo nuevo/conocido
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ AudioUtils.kt
â”‚   â”‚       â”œâ”€â”€ ImageUtils.kt
â”‚   â”‚       â””â”€â”€ Logger.kt
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â””â”€â”€ facenet.tflite              # Modelo FaceNet (embeddings 128D, ~20MB)
â”‚   â””â”€â”€ res/
â”‚       â”œâ”€â”€ layout/
â”‚       â”‚   â”œâ”€â”€ activity_main.xml
â”‚       â”‚   â””â”€â”€ robot_face_view.xml
â”‚       â”œâ”€â”€ values/
â”‚       â”‚   â”œâ”€â”€ strings.xml
â”‚       â”‚   â”œâ”€â”€ colors.xml
â”‚       â”‚   â””â”€â”€ themes.xml
â”‚       â”œâ”€â”€ raw/
â”‚       â”‚   â””â”€â”€ hey_robi_wake.ppn
â”‚       â””â”€â”€ xml/
â”‚           â””â”€â”€ network_security_config.xml  # Config de certificate pinning
â””â”€â”€ build.gradle.kts
```

### 4.3 Ciclo de Vida de la AplicaciÃ³n

```mermaid
stateDiagram-v2
    [*] --> AppLaunch
    AppLaunch --> PermissionsCheck
    
    PermissionsCheck --> RequestPermissions: Faltan permisos
    PermissionsCheck --> StartService: Permisos OK
    
    RequestPermissions --> PermissionsCheck: Usuario concede
    RequestPermissions --> [*]: Usuario rechaza
    
    StartService --> ServiceRunning
    
    state ServiceRunning {
        [*] --> IDLE
        IDLE --> LISTENING: Wake Word Detectado
        LISTENING --> SEARCHING: Audio capturado\\ CÃ¡mara activada
        SEARCHING --> GREETING: Usuario reconocido
        SEARCHING --> REGISTERING: Usuario desconocido
        SEARCHING --> IDLE: Timeout sin rostro (5s)
        GREETING --> IDLE: Saludo completado
        REGISTERING --> LISTENING: Pregunta nombre
        LISTENING --> THINKING: Nombre recibido
        THINKING --> RESPONDING: Response recibido
        RESPONDING --> IDLE: AnimaciÃ³n completa
        
        LISTENING --> ERROR: Timeout
        SEARCHING --> ERROR: Error de cÃ¡mara
        THINKING --> ERROR: Error de Red
        ERROR --> IDLE: Retry / Timeout
    }
    
    ServiceRunning --> ActivityVisible: Usuario abre app
    ActivityVisible --> ServiceRunning: Usuario cierra app
    
    ServiceRunning --> [*]: Usuario detiene servicio
```

### 4.4 Servicio en Foreground

#### Responsabilidades
- Mantener wake word detector activo 24/7
- Gestionar conexiÃ³n WebSocket con backend (persistente)
- Gestionar conexiÃ³n Bluetooth con ESP32
- Enviar heartbeat BLE al ESP32 cada 1 segundo
- Mostrar notificaciÃ³n persistente
- MÃ­nimo consumo de baterÃ­a en reposo
- Reinicio automÃ¡tico si el sistema lo mata

#### Watchdog Externo (ServiceWatchdog)

El sistema operativo Android es agresivo al matar servicios en segundo plano para ahorrar baterÃ­a. No se puede confiar Ãºnicamente en el flag `START_STICKY` para garantizar que el servicio se reinicie. Por eso se implementa un **watchdog externo**:

```
Mecanismo: AlarmManager con alarma exacta cada 60 segundos
FunciÃ³n: Verificar si RobotService estÃ¡ vivo
AcciÃ³n si muerto: Forzar reinicio del servicio
Independencia: El watchdog es un BroadcastReceiver separado,
              no depende del servicio que supervisa
Consumo: Despreciable (~0.1% baterÃ­a/hora)

Flujo:
1. AlarmManager dispara WatchdogReceiver cada 60s
2. WatchdogReceiver verifica si RobotService estÃ¡ corriendo
3. Si no estÃ¡ corriendo â†’ startForegroundService()
4. Si estÃ¡ corriendo â†’ no hacer nada
5. Reprogramar siguiente alarma
```

Esto cambia la filosofÃ­a de "Android es el controlador confiable" a "Android es un componente que necesita su propia supervisiÃ³n".

#### NotificaciÃ³n Persistente

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤–  Robot Asistente                â”‚
â”‚  Estado: Esperando comando          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  [ConfiguraciÃ³n]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Nota: El robot se controla ÃšNICAMENTE por voz.
No hay botÃ³n de control disponible para el usuario.
La notificaciÃ³n es solo informativa.
```

### 4.5 Detector de Wake Word

#### ConfiguraciÃ³n Porcupine

```
Wake Word: "Hey Robi"
Sensibilidad: 0.7 (balance entre falsos positivos/negativos)
Modelo: Porcupine (hasta 3 palabras gratis)
Procesamiento: 100% local (sin internet)
Consumo CPU: <2% en reposo
Latencia detecciÃ³n: <100ms
Archivo modelo: hey_robi_wake.ppn
```

#### Modo de Escucha Continua (ConversaciÃ³n Fluida)

Una vez detectado el wake word y completada la primera interacciÃ³n, el robot entra en **modo de escucha continua** durante `CONVERSATION_KEEP_ALIVE_MS` (60 segundos por defecto). Durante este perÃ­odo, el usuario puede seguir hablando sin necesidad de repetir "Hey Robi":

```
Modo de escucha continua:
  DuraciÃ³n: 60 segundos desde la Ãºltima interacciÃ³n (parÃ¡metro ajustable)
  Comportamiento: Android escucha automÃ¡ticamente cada vez que el usuario habla
  DetecciÃ³n de silencio: 2s de silencio â†’ grabar y enviar al backend
  Estado visual: ğŸ‘‚ (LISTENING) con indicador de countdown
  Timeout: Al cumplirse los 60s sin actividad â†’ volver a IDLE (ğŸ¤–)
  Wake word: Vuelve a ser necesario solo tras volver a IDLE
  
  Equilibrio:
    - No tan corto (<30s) â†’ interrupciones frecuentes e incomodas
    - No tan largo (>120s) â†’ robot escucha innecesariamente
    - 60s es el valor inicial recomendado; ajustar segÃºn experiencia
```

#### Flujo de DetecciÃ³n y ActivaciÃ³n

```mermaid
sequenceDiagram
    participant U as Usuario
    participant P as Porcupine
    participant S as StateManager
    participant FR as FaceRecognition
    participant E as ESP32
    participant A as Activity

    loop Escucha Continua (Wake Word activo)
        P->>P: Procesar buffer de audio
    end

    U->>P: "Hey Robi"
    P->>P: Detecta keyword
    P->>S: onWakeWordDetected()

    S->>S: Cambiar estado a LISTENING
    S->>A: Lanzar/Traer al frente
    A->>A: Mostrar cara escuchando (ğŸ‘‚) [INMEDIATO]
    Note over A: TransiciÃ³n visual instantÃ¡nea

    S->>S: Cambiar estado a SEARCHING
    A->>A: Mostrar cara buscando (ğŸ”)
    A->>E: BLE: search_rotate(Â±90Â°, speed=30) â€” buscar persona
    A->>FR: Activar CÃMARA FRONTAL (siempre delantera)
    FR->>FR: ML Kit: Detectar rostro en frames

    alt Rostro detectado antes de PERSON_SEARCH_TIMEOUT_MS (8s)
        A->>E: BLE: stop() â€” dejar de rotar
        FR->>FR: Extraer embedding FaceNet TFLite
        FR->>S: onFaceDetected(embedding, frame)
        S->>S: Cambiar estado a GREETING o REGISTERING
    else Timeout sin rostro (8s)
        A->>E: BLE: stop() â€” dejar de rotar
        FR->>S: onFaceTimeout()
        S->>S: Estado LISTENING
        A->>A: TTS: "No puedo verte. Por favor acÃ©rcate al robot"
        S->>S: Volver a IDLE tras mensaje
    end

    Note over A: Tras interacciÃ³n: modo escucha continua 60s
    Note over A: El usuario puede seguir hablando sin wake word
```

### 4.6 Reconocimiento Facial On-Device (Android)

#### DecisiÃ³n TecnolÃ³gica: ML Kit + TFLite FaceNet

El reconocimiento facial se realiza **completamente en el dispositivo Android**, sin necesidad de enviar imÃ¡genes al backend. Se usa **exclusivamente la cÃ¡mara frontal (delantera)**, que es la que el usuario siempre tiene de frente. La cÃ¡mara trasera estÃ¡ deshabilitada para este mÃ³dulo.

**EvaluaciÃ³n de alternativas:**

| OpciÃ³n | Ventajas | Desventajas | Veredicto |
|--------|----------|-------------|-----------|
| Backend (face_recognition/DeepFace) | Alta precisiÃ³n | Latencia de red (500ms+), falla sin WiFi | âŒ Eliminado |
| OpenCV + LBPH | Sin dependencias externas | Baja precisiÃ³n en condiciones reales | âŒ Descartado |
| ML Kit Face Detection only | Simple, oficial de Google | Solo detecta caras, no las identifica | âš ï¸ Usado solo para detecciÃ³n |
| TFLite MobileNet + ArcFace | Alta precisiÃ³n, on-device | Modelo grande (~80MB) | âœ… Seleccionado como modelo |
| TFLite FaceNet (Google) | On-device, 128D embeddings, rÃ¡pido (<200ms), modelo liviano (~20MB), probado en Android 7+ | PrecisiÃ³n ligeramente menor que ArcFace | âœ… **Seleccionado como implementaciÃ³n** |

**DecisiÃ³n final: ML Kit Face Detection + TFLite FaceNet (cÃ¡mara frontal)**

- **ML Kit Face Detection**: Detecta bounding boxes en tiempo real desde la cÃ¡mara frontal.
- **TFLite FaceNet**: Genera embedding 128D. Se compara con SQLite local.
- **Umbral de reconocimiento**: Similitud coseno > 0.70 = mismo individuo (configurable).
- **CÃ¡mara**: Siempre la cÃ¡mara frontal (LENS_FACING_FRONT). La trasera no se usa.

#### Arquitectura del MÃ³dulo

```mermaid
flowchart TD
    CAM[Frames de CÃ¡mara] --> MLKIT[ML Kit FaceDetector\nDetecta bounding box del rostro]
    MLKIT -->|Sin rostro| WAIT[Esperar siguiente frame]
    WAIT --> CAM
    MLKIT -->|Rostro detectado| CROP["Recortar y normalizar ROI\n112x112px, RGB, [-1,1]"]
    CROP --> FACENET[TFLite FaceNet\nInferencia on-device]
    FACENET --> EMBED[Embedding 128D]
    EMBED --> SEARCH[FaceSimilarityEngine\nBuscar en SQLite local]
    SEARCH -->|Similitud > 0.70| MATCH[Usuario reconocido\nRetornar user_id + nombre + score]
    SEARCH -->|Similitud <= 0.70| UNKNOWN[Usuario desconocido\nRetornar unknown]
    MATCH --> ORCHESTRATOR[GreetingOrchestrator]
    UNKNOWN --> ORCHESTRATOR
```

#### Almacenamiento de Embeddings (SQLite Local Android)

```sql
-- Tabla en Room Database local del dispositivo Android
CREATE TABLE face_embeddings (
    id          INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id     TEXT NOT NULL UNIQUE,
    name        TEXT NOT NULL,
    embedding   BLOB NOT NULL,   -- Float array 128D serializado
    created_at  INTEGER NOT NULL,
    last_seen   INTEGER
);
```

Los embeddings se sincronizan tambiÃ©n con el backend (tabla `users.face_embedding`) despuÃ©s del registro, de modo que si la app se reinstala, los datos se recuperan del servidor.

#### ParÃ¡metros de Reconocimiento

```
Modelo TFLite: facenet_512.tflite (versiÃ³n 512D) o facenet.tflite (128D)
Input: 112x112 RGB normalizado [-1, 1]
Output: Vector float32 de 128 o 512 dimensiones (L2-normalizado)
MÃ©trica de comparaciÃ³n: Similitud coseno
Umbral de aceptaciÃ³n: 0.70 (configurable en AppPreferences)
LibrerÃ­a de detecciÃ³n: ML Kit Face Detection API (com.google.mlkit:face-detection)
Procesamiento de imagen: CameraX + ImageAnalysis para frames en tiempo real
FPS anÃ¡lisis: ~10 fps (suficiente para detecciÃ³n rÃ¡pida sin saturar CPU)
Latencia total (detecciÃ³n + embedding): <200ms
```

#### Flujo de Registro de Nueva Persona

Cuando el usuario no es reconocido, el backend pregunta su nombre y el resultado se guarda simultÃ¡neamente en el backend y en el dispositivo:

```mermaid
sequenceDiagram
    participant A as App Android
    participant FR as FaceRecognitionManager
    participant B as Backend
    participant DB as SQLite Local

    A->>FR: captureRegistrationFrame()
    Note over FR: ML Kit detecta rostro<br/>mÃ¡s grande del frame
    FR->>FR: FaceNet genera embedding 128D
    FR->>A: embedding + frame JPEG

    A->>B: WS: interaction_start (user_id=unknown) + imagen

    B-->>A: "WS: Audio Â¿CÃ³mo te llamas?"
    A->>B: WS: Audio con nombre del usuario

    Note right of B: Gemini extrae nombre del audio
    B->>B: Genera user_id Ãºnico
    B->>B: Guarda en tabla users
    B-->>A: "WS: {type: user_registered, user_id, name}"

    A->>FR: saveEmbedding(user_id, name, embedding)
    FR->>DB: INSERT INTO face_embeddings
    
    B-->>A: "WS: text_chunk Mucho gusto, [nombre]!"
    A->>A: Android TTS reproduce bienvenida
```

### 4.7 Flujo de Saludo Inicial Completo

Este flujo describe en detalle cÃ³mo el robot saluda a cada persona despuÃ©s de detectar el wake word, cubriendo los cuatro caminos posibles:

```mermaid
flowchart TD
    WW[Wake Word: Hey Robi detectado] --> LISTEN["Estado LISTENING\nMostrar ğŸ‘‚ inmediato"]
    LISTEN --> CAMERA["Activar CÃMARA FRONTAL\nSeÃ±al ESP32: search_rotate Â±90Â°\nEstado SEARCHING ğŸ”"]
    CAMERA --> DETECT{ML Kit detecta\nrostro en frame}

    DETECT -->|No, continÃºa...| TIMER{"Â¿Timeout PERSON_SEARCH_TIMEOUT_MS\n(8s por defecto)?"}
    TIMER -->|No| DETECT
    TIMER -->|SÃ­| STOP_SEARCH["ESP32: stop()\nDetener bÃºsqueda"]
    STOP_SEARCH --> NO_FACE["TTS: 'No puedo verte.\nPor favor acÃ©rcate al robot'\nEstado IDLE ğŸ¤–"]

    DETECT -->|SÃ­, rostro detectado| STOP_ROT["ESP32: stop() â€” dejar de rotar"]
    STOP_ROT --> EMBED[FaceNet: Generar embedding]
    EMBED --> MATCH{"Similitud coseno\nvs BD local"}

    MATCH -->|Score > 0.70\nUsuario conocido| GREET_KNOWN["Estado GREETING ğŸ‘‹\nEnviar user_id al backend"]
    MATCH -->|Score <= 0.70\nUsuario desconocido| ASK_NAME["Estado REGISTERING â“\nEnviar unknown al backend"]

    GREET_KNOWN --> BACKEND_GREET["Backend genera\nHola [nombre] con LLM + TTS"]
    BACKEND_GREET --> PLAY_GREET["Robot reproduce saludo\nemotion:greeting + voz"]
    PLAY_GREET --> READY["Estado LISTENING ğŸ‘‚\nModo escucha continua 60s\nListo para recibir Ã³rdenes sin wake word"]

    ASK_NAME --> BACKEND_ASK["Backend genera\nÂ¿CÃ³mo te llamas? con TTS"]
    BACKEND_ASK --> PLAY_ASK["Robot reproduce pregunta\nemotion:curious"]
    PLAY_ASK --> LISTEN2["Estado LISTENING ğŸ‘‚\nGraba respuesta del usuario"]
    LISTEN2 --> STT[Gemini extrae nombre del audio]
    STT --> REGISTER[Guardar usuario en backend + SQLite local]
    REGISTER --> BACKEND_WELCOME["Backend genera\nMucho gusto [nombre]! con TTS"]
    BACKEND_WELCOME --> PLAY_WELCOME["Robot reproduce bienvenida\nemotion:happy"]
    PLAY_WELCOME --> READY
```

### 4.8 GrabaciÃ³n y Procesamiento de Audio

#### Especificaciones TÃ©cnicas

```
Formato: AAC (compresiÃ³n eficiente)
Sample Rate: 16kHz (suficiente para voz)
Bitrate: 64kbps
Channels: Mono
Buffer: 1024 frames
DetecciÃ³n de silencio: 2 segundos de silencio â†’ fin
Timeout mÃ¡ximo: 10 segundos
```

#### Flujo de GrabaciÃ³n

```mermaid
flowchart TD
    START[Iniciar GrabaciÃ³n] --> RECORD[Grabar audio en buffer]
    RECORD --> ANALYZE[Analizar volumen]
    
    ANALYZE --> CHECK{Â¿Silencio?}
    CHECK -->|No| RECORD
    CHECK -->|SÃ­ >2s| STOP[Detener grabaciÃ³n]
    
    ANALYZE --> TIMEOUT{Â¿Timeout 10s?}
    TIMEOUT -->|No| RECORD
    TIMEOUT -->|SÃ­| STOP
    
    STOP --> COMPRESS[Comprimir a AAC]
    COMPRESS --> SEND[Enviar a backend vÃ­a WebSocket]
```

### 4.8b Captura de Foto o Video por Comando de Voz

Cuando el usuario solicita explÃ­citamente tomar una foto o grabar un video (p. ej., "Hey Robi, toma una foto y dime quÃ© ves" o "Hey Robi, graba un video de diez segundos y comÃ©ntame"), el robot activa la cÃ¡mara, captura el contenido y lo adjunta al mensaje enviado a Gemini.

#### Flujo de Captura y EnvÃ­o

```mermaid
flowchart TD
    WW[Wake Word detectado] --> LISTEN[Estado LISTENING ğŸ‘‚]
    LISTEN --> RECORD_CMD[Grabar comando del usuario]
    RECORD_CMD --> SEND_AUDIO[Enviar audio + interaction_start al backend]
    SEND_AUDIO --> GEMINI_INTENT{Gemini detecta intenciÃ³n\nde captura en el audio}

    GEMINI_INTENT -->|photo_request| NOTIFY_CAP["WS: capture_request\ntipo=photo"]
    GEMINI_INTENT -->|video_request| NOTIFY_CAP2["WS: capture_request\ntipo=video, duration_ms"]
    GEMINI_INTENT -->|otro| NORMAL[Flujo de respuesta normal]

    NOTIFY_CAP --> APP_PHOTO["App activa cÃ¡mara FRONTAL\nCaptura foto JPEG"]
    NOTIFY_CAP2 --> APP_VIDEO["App activa cÃ¡mara FRONTAL\nGraba video MP4"]

    APP_PHOTO --> SEND_IMG["WS: image (base64 JPEG)\npurpose=context"]
    APP_VIDEO --> SEND_VID["WS: video (base64 MP4)\nduration_ms"]

    SEND_IMG --> GEMINI_RESP[Gemini procesa audio + imagen/video]
    SEND_VID --> GEMINI_RESP

    GEMINI_RESP --> STREAM_RESP["WS: emotion + text_chunks\nAndroid TTS reproduce"]
    NORMAL --> STREAM_RESP
```

#### Mensaje del Servidor: Solicitud de Captura

```json
// Enviado por el backend cuando Gemini detecta intenciÃ³n de captura
{
  "type": "capture_request",
  "request_id": "uuid-v4",
  "capture_type": "photo",      // "photo" | "video"
  "duration_ms": null           // Solo para video (ej: 10000 = 10s)
}
```

#### Especificaciones de Captura

```
Foto:
  Formato: JPEG, calidad 85%
  ResoluciÃ³n: 1280x720 (suficiente para anÃ¡lisis visual)
  CÃ¡mara: FRONTAL (siempre delantera â€” la trasera nunca se usa)
  Tiempo de captura: <500ms
  TamaÃ±o mÃ¡ximo: 500KB (despuÃ©s de compresiÃ³n)

Video:
  Formato: MP4 (H.264)
  ResoluciÃ³n: 1280x720
  FPS: 30
  DuraciÃ³n mÃ¡xima: 30 segundos
  TamaÃ±o mÃ¡ximo: 20MB
  Audio: Opcional (sin audio por defecto para reducir tamaÃ±o)
  CÃ¡mara: FRONTAL (siempre delantera)
  Tiempo de captura: SegÃºn duraciÃ³n solicitada
```

#### Restricciones y Manejo de Errores

```
- Si la cÃ¡mara estÃ¡ ocupada (reconocimiento facial activo):
  Finalizar bÃºsqueda facial â†’ iniciar captura
- Si no hay permiso de cÃ¡mara:
  Android TTS: "Necesito permiso de cÃ¡mara para tomar fotos"
- Si el video excede el tamaÃ±o mÃ¡ximo:
  Recortar a los primeros N segundos dentro del lÃ­mite
- Si el backend no responde capture_request en 10s:
  Abortar captura â†’ respuesta sin imagen
```

### 4.8c Android TextToSpeech (TTS del Sistema)

El robot utiliza el **Android TextToSpeech** integrado en el sistema operativo para reproducir las respuestas de texto que llegan del backend en streaming. No se genera audio en el servidor, lo que elimina latencia de red y dependencias de servicios externos de TTS.

#### Funcionamiento

```
Motor TTS: Android TTS del sistema (configurable por idioma)
Entrada: text_chunk messages del WebSocket (texto en streaming)
AcumulaciÃ³n: Los chunks se acumulan en un buffer de oraciones
ReproducciÃ³n: Al detectar el fin de una oraciÃ³n (punto, salto de lÃ­nea)
              el buffer se envÃ­a al TTS para sÃ­ntesis inmediata
Ventaja: El robot empieza a hablar con el primer chunk recibido,
         sin esperar el texto completo
```

#### ConfiguraciÃ³n Disponible (AppPreferences)

```
tts_language: String (ej: "es", "en", "fr") â€” idioma de la voz
tts_voice_name: String â€” nombre de la voz del sistema (si mÃºltiples disponibles)
tts_speech_rate: Float (0.5 - 2.0, default: 0.9) â€” velocidad (ligeramente menor
                 que la normal para mayor claridad en voz robÃ³tica)
tts_pitch: Float (0.5 - 2.0, default: 1.0) â€” tono de la voz
tts_audio_focus: Boolean (default: true) â€” solicitar foco de audio antes de hablar
```

#### Flujo de ReproducciÃ³n en Streaming

```mermaid
flowchart TD
    WS[WS: text_chunk recibido] --> BUFFER[Acumular en buffer de oraciÃ³n]
    BUFFER --> CHECK{"Â¿Fin de oraciÃ³n detectado?\n(punto, signo de excl., etc.)"}
    CHECK -->|No| WS
    CHECK -->|SÃ­| TTS["Android TTS: speak(buffer)"]
    TTS --> CLEAR[Limpiar buffer]
    CLEAR --> WS

    WS2[WS: stream_end recibido] --> FLUSH[Enviar buffer restante al TTS]
    FLUSH --> IDLE[Estado IDLE]
```

### 4.9 Cliente WebSocket (ComunicaciÃ³n Streaming con Backend)

#### ConfiguraciÃ³n

```
URL: wss://192.168.2.200:9393/ws/interact (WebSocket sobre TLS, gestionado por Nginx)
Ping Interval: 30s
Reconnect Policy: Backoff exponencial (1s, 2s, 4s, 8s, mÃ¡x 30s)
Certificate Pinning: Habilitado (fingerprint del cert del servidor)
Headers:
  - X-API-Key: <clave-configurada> (en handshake)
  - User-Agent: RobotAndroid/1.4
```

#### API REST Auxiliar (para operaciones no-streaming)

```
Base URL: https://192.168.2.200:9393 (HTTPS obligatorio, vÃ­a Nginx)
Uso: Health checks, gestiÃ³n de usuarios
Timeout ConexiÃ³n: 10s
Timeout Lectura: 30s
Certificate Pinning: Habilitado
```

#### Manejo de Mensajes WebSocket

```mermaid
flowchart TD
    CONNECT[Conectar WebSocket WSS] --> AUTH[Enviar auth message]
    AUTH --> WAIT_AUTH{auth_ok?}
    
    WAIT_AUTH -->|SÃ­| READY[ConexiÃ³n lista]
    WAIT_AUTH -->|No/Timeout| RETRY[Reintentar conexiÃ³n]
    RETRY --> CONNECT
    
    READY --> IDLE[Esperando interacciÃ³n]
    
    IDLE -->|Wake word| SEND_START[Enviar interaction_start]
    SEND_START --> STREAM_AUDIO["Enviar audio grabado (binario)"]
    STREAM_AUDIO --> SEND_END[Enviar audio_end]
    
    SEND_END --> RECV{Recibir mensajes}
    
    RECV -->|emotion| UPDATE_FACE[Actualizar cara INMEDIATO]
    RECV -->|text_chunk| SPEAK[Android TTS: reproducir texto chunk]
    RECV -->|capture_request| CAPTURE[Activar cÃ¡mara + capturar]
    RECV -->|response_meta| SHOW_EMOJIS[Mostrar secuencia emojis]
    RECV -->|stream_end| COMPLETE[InteracciÃ³n completa]
    RECV -->|error| HANDLE_ERROR[Manejar error]
    
    UPDATE_FACE --> RECV
    SPEAK --> RECV
    CAPTURE --> RECV
    SHOW_EMOJIS --> RECV
    COMPLETE --> IDLE
    
    HANDLE_ERROR --> ERROR_STATE[Estado ERROR]
    ERROR_STATE --> IDLE
```

#### Manejo de DesconexiÃ³n WebSocket

```
DesconexiÃ³n detectada:
1. Marcar estado como OFFLINE
2. Mostrar banner "Backend no disponible"
3. Iniciar reconesiÃ³n automÃ¡tica con backoff
4. Si reconecta: enviar auth, restaurar estado IDLE
5. Wake word sigue activo durante desconexiÃ³n
   (audio se graba y envÃ­a cuando reconecte)
```

### 4.10 GestiÃ³n de Expresiones Visuales

#### DiseÃ±o de Interfaz (Landscape, Tema Oscuro)

La aplicaciÃ³n tiene un diseÃ±o fijo en orientaciÃ³n **landscape (horizontal)**, bloqueado para que nunca rote a vertical. El fondo es negro puro (tema oscuro). Solo se muestran dos elementos principales:

```
Layout en landscape (pantalla completa):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”‹10% ğŸ¤–  [superior izq â€” solo si baterÃ­a robot â‰¤15%]    [bat. celular âš¡85%]  â”‚
â”‚                                                            â”‚
â”‚                                                            â”‚
â”‚              [EMOJI â€” 80% de la pantalla]                  â”‚
â”‚                 centrado vertical y horizontal              â”‚
â”‚                    con animaciÃ³n                           â”‚
â”‚                                                            â”‚
â”‚                                                            â”‚
â”‚        [texto del robot â€” 10% de altura inferior]          â”‚
â”‚     azul claro metalizado Â· solo ayuda / subtÃ­tulo         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Indicadores de baterÃ­a (parpadeantes lentos, pequeÃ±os, no intrusivos):
- BaterÃ­a robot (ESP32 telemetrÃ­a) â‰¤15%: esquina superior izquierda
    â†’ Icono rojo ğŸ”‹ + icono robot ğŸ¤– + porcentaje restante, titilando lento
    â†’ No se muestra si baterÃ­a > 15%
- BaterÃ­a del celular: esquina superior derecha
    â†’ Icono naranja claro âš¡ + porcentaje, titilando lento
    â†’ Se muestra siempre (actualizado desde BatteryManager del sistema)

No hay botones de control en pantalla. El robot solo se controla por voz.
```

#### Colores y Estilos

```
Fondo: #000000 (negro puro)
Texto de respuesta: #88CCEE o similar azul claro metalizado
  â†’ Fuente: monospace o sans-serif medium, tamaÃ±o legible en landscape
Emoji: ImÃ¡genes OpenMoji cargadas desde CDN, tamaÃ±o mÃ¡ximo en su contenedor 80%
Indicador baterÃ­a robot: #FF3333 (rojo vivo), opacidad pulsante 0.4â†’1.0
Indicador baterÃ­a celular: #FFAA44 (naranja claro), opacidad pulsante 0.4â†’1.0
```

#### TransiciÃ³n Inmediata al Detectar Wake Word

El robot debe cambiar su expresiÃ³n visual **al instante** de detectar el wake word, sin esperar a que el audio se grabe completo. Esto gestiona las expectativas del usuario: incluso si hay un pequeÃ±o delay de red, el usuario ve al robot reaccionar y eso es mucho mÃ¡s tolerable que una mirada en blanco.

#### Estados Visuales

```
IDLE (Reposo):
  Emoji: ğŸ¤– (1F916)
  AnimaciÃ³n: Parpadeo suave cada 3-5s
  TransiciÃ³n: Ninguna

LISTENING (Escuchando) [TransiciÃ³n INMEDIATA al wake word]:
  Emoji: ğŸ‘‚ (1F442)
  AnimaciÃ³n: Pulso suave (escala 1.0 â†’ 1.1 â†’ 1.0)
  Indicador: Onda de audio visual (sutil, en texto inferior)

SEARCHING (Buscando persona con cÃ¡mara frontal):
  Emoji: ğŸ” (1F50D)
  AnimaciÃ³n: RotaciÃ³n lenta del emoji (simulando escaneo)
  DuraciÃ³n: Hasta rostro detectado o timeout PERSON_SEARCH_TIMEOUT_MS (8s)
  Nota: El robot fÃ­sicamente rota Â±90Â° y se mueve buscando a la persona

GREETING (Saludando a usuario reconocido):
  Emoji: ğŸ‘‹ (1F44B) â†’ luego emotion tag del LLM (greeting)
  AnimaciÃ³n: Wave + bounce entrada
  TransiciÃ³n: Inmediata al recibir emotion tag del backend

REGISTERING (Registrando nueva persona):
  Emoji: â“ (2753)
  AnimaciÃ³n: Pulso suave
  DuraciÃ³n: Hasta completar registro

THINKING (Procesando):
  Emoji: ğŸ¤” (1F914)
  AnimaciÃ³n: RotaciÃ³n suave del emoji
  Indicador: Texto inferior "Pensando..."

EMOTION (EmociÃ³n recibida del LLM vÃ­a WebSocket):
  Emoji: SegÃºn emotion tag (ver mapeo en secciÃ³n 3.7)
  TransiciÃ³n: Se muestra ANTES de que el TTS empiece a hablar
  DuraciÃ³n: Hasta que termina la reproducciÃ³n del TTS
  SincronizaciÃ³n: La cara siempre coincide con la intenciÃ³n de la respuesta
  Texto inferior: texto de respuesta (subtÃ­tulos en azul claro metalizado)

RESPONDING (Respondiendo con secuencia de emojis):
  Secuencia: Hasta 3 emojis contextuales (de response_meta)
  DuraciÃ³n: 2s por emoji (configurable)
  TransiciÃ³n: fade | slide | bounce
  Audio: Android TTS sintetiza en tiempo real a medida que llegan text_chunks
  Texto inferior: texto de respuesta en rolling (Ãºltimas palabras), azul metalizado

MOVING (Ejecutando secuencia de movimientos del robot):
  Emoji: El indicado en emotion_during del move_sequence
  DuraciÃ³n: Sincronizado con total_duration_ms de la secuencia completa
  Texto inferior: DescripciÃ³n de la acciÃ³n ("Rotando hacia la derecha...")

ERROR (Error):
  Emoji: ğŸ˜• (1F615)
  AnimaciÃ³n: Shake
  DuraciÃ³n: 2s â†’ vuelve a IDLE

DISCONNECTED (Cerebro desconectado):
  Emoji: ğŸ”Œ (1F50C)
  AnimaciÃ³n: Parpadeo lento
  DuraciÃ³n: Hasta reconexiÃ³n
```

#### ImplementaciÃ³n de Animaciones (Streaming)

```mermaid
graph LR
    A[WS: emotion tag recibido] --> B[Actualizar cara INMEDIATO]
    B --> C[WS: text_chunks llegando]
    C --> D[Android TTS reproduce en tiempo real]
    D --> E[WS: response_meta recibido]
    E --> F[Extraer secuencia emojis]
    F --> G[Emoji 1]
    G --> H[AnimaciÃ³n entrada + Display 2s]
    H --> I{Â¿MÃ¡s emojis?}
    I -->|SÃ­| G
    I -->|No| J[Delay 500ms]
    J --> K[Volver a IDLE]
```

#### Emojis OpenMoji: estrategia de carga

Los emojis de OpenMoji **no requieren descarga previa en archivo ZIP**. El LLM ya conoce todos los cÃ³digos Unicode de los emojis que necesita usar, y Android descarga los archivos SVG directamente desde el CDN de OpenMoji (`https://openmoji.org/data/color/svg/<HEXCODE>.svg`) en tiempo real. Solo se pre-cargan en cachÃ© local los 20 emojis mÃ¡s frecuentes (estados fijos y emociones mÃ¡s comunes) al iniciar la app. El resto se descargan y cachean automÃ¡ticamente la primera vez que se necesitan.

```
Pre-carga inicial (20 emojis mÃ¡s usados):
  Estados: 1F916, 1F442, 1F914, 1F615, 1F50C, 1F50D, 1F44B, 2753
  Emociones: 1F600, 1F603, 1F622, 1F97A, 1F632, 2764-FE0F, 1F60E, 1F44D
  Estrategia cachÃ©: LRU, mÃ¡ximo 50MB, directorio /cache/openmoji/
CDN base: https://openmoji.org/data/color/svg/
Backend no necesita los archivos de emojis â€” solo envÃ­a cÃ³digos hexadecimales.
```

### 4.11 Gestor de Bluetooth (ComunicaciÃ³n con ESP32)

#### Protocolo de ComunicaciÃ³n

```
Transport: Bluetooth Low Energy (BLE)
Service UUID: 6E400001-B5A3-F393-E0A9-E50E24DCCA9E (Nordic UART Service)
TX Characteristic: 6E400002-B5A3-F393-E0A9-E50E24DCCA9E (Write)
RX Characteristic: 6E400003-B5A3-F393-E0A9-E50E24DCCA9E (Notify)

Formato de mensajes: JSON UTF-8
MTU: 512 bytes
```

#### Comandos Bluetooth

```json
// Heartbeat (enviado cada 1 segundo)
{
  "type": "heartbeat",
  "timestamp": 1234567890
}

// Movimiento
{
  "type": "move",
  "direction": "forward|backward|left|right|stop",
  "speed": 0-100,
  "duration": milliseconds
}

// Control de luces
{
  "type": "light",
  "action": "on|off|blink",
  "color": "red|green|blue|white|rgb(r,g,b)",
  "intensity": 0-100
}

// Solicitar telemetrÃ­a
{
  "type": "telemetry",
  "request": "sensors|battery|status"
}
```

#### Respuestas del ESP32

```json
// ConfirmaciÃ³n de comando
{
  "status": "ok|error",
  "command_id": "uuid",
  "error_msg": "descripciÃ³n si hay error"
}

// TelemetrÃ­a de sensores
{
  "type": "telemetry",
  "battery": 75,
  "sensors": {
    "distance_front": 150,
    "distance_rear": 200,
    "cliff_detected": false,
    "light_level": 300
  },
  "timestamp": 1234567890
}
```

### 4.12 GestiÃ³n de Permisos

#### Permisos Requeridos

```
REQUIRED (runtime):
- RECORD_AUDIO: GrabaciÃ³n de voz
- CAMERA: Captura de imÃ¡genes/video
- BLUETOOTH: ConexiÃ³n con ESP32
- BLUETOOTH_CONNECT: Android 12+
- BLUETOOTH_SCAN: Android 12+

OPTIONAL:
- FOREGROUND_SERVICE: Servicio persistente
- WAKE_LOCK: Mantener CPU para wake word
- INTERNET: ComunicaciÃ³n con backend
```

#### Flujo de Solicitud

```mermaid
sequenceDiagram
    participant A as App
    participant S as Sistema Android
    participant U as Usuario
    
    A->>S: Solicitar RECORD_AUDIO
    S->>U: Dialog de permiso
    U->>S: Conceder/Denegar
    
    alt Permiso concedido
        S->>A: Permission Granted
        A->>A: Continuar setup
    else Permiso denegado
        S->>A: Permission Denied
        A->>U: Explicar necesidad del permiso
        A->>A: BotÃ³n para abrir Settings
    end
```

### 4.13 Persistencia Local

#### Datos Almacenados

```
EncryptedSharedPreferences (Android Keystore):
- api_key: String (encriptado)
- backend_url: String
- server_cert_fingerprint: String (para certificate pinning)
- user_id_default: String
- wake_word_sensitivity: Float
- face_recognition_threshold: Float (default: 0.70) // Umbral ML Kit on-device (Android)
- face_search_timeout_ms: Int (default: 8000)       // PERSON_SEARCH_TIMEOUT_MS
- emoji_cache_enabled: Boolean
- bluetooth_device_mac: String
- last_sync: Long (timestamp)
- tts_language: String (default: "es") // Idioma del TTS
- tts_voice_name: String (default: "") // Voz del sistema (vacÃ­o = default del sistema)
- tts_speech_rate: Float (default: 0.9) // Velocidad de habla
- tts_pitch: Float (default: 1.0)       // Tono de la voz

Room Database Local (face_embeddings.db):
- Tabla: face_embeddings (user_id, name, embedding BLOB 128D, created_at, last_seen)
- UbicaciÃ³n: /data/data/com.robot/databases/face_embeddings.db
- PropÃ³sito: Reconocimiento facial on-device sin conexiÃ³n al backend
- SincronizaciÃ³n: Se sincroniza con el backend al registrar un nuevo usuario

Cache de Emojis:
- Directorio: /data/data/com.robot/cache/openmoji/
- Formato: SVG files (*.svg)
- TamaÃ±o mÃ¡ximo: 50MB
- Estrategia: LRU (Least Recently Used)
- Pre-carga: 20 emojis mÃ¡s comunes
```

### 4.14 ConfiguraciÃ³n de CompilaciÃ³n

```
minSdkVersion: 24 (Android 7.0)
targetSdkVersion: 33 (Android 13)
compileSdkVersion: 33

Optimizaciones:
- R8/ProGuard: Habilitado
- Multidex: Habilitado
- ViewBinding: Habilitado
- Coroutines: Para operaciones async

Dependencias principales:
- Kotlin Coroutines
- OkHttp (WebSocket client + Certificate Pinning)
- Retrofit + OkHttp (REST auxiliar)
- Coil (carga de imÃ¡genes/SVG)
- Porcupine (wake word: hey_robi_wake.ppn)
- AndroidX (Lifecycle, WorkManager)
- EncryptedSharedPreferences (seguridad)
- CameraX (captura de frames para reconocimiento facial + foto/video por comando)
- ML Kit Face Detection (com.google.mlkit:face-detection:16.x)
- TensorFlow Lite (com.google.ai.edge.litert:litert:1.x)
- Room Database (SQLite local para embeddings faciales)
- Modelo TFLite: facenet.tflite (~20MB, incluido en assets/)
- Android TextToSpeech (API del sistema, sin dependencia externa)
  Nota: No se requieren librerÃ­as adicionales; TextToSpeech estÃ¡ incluido
  en android.speech.tts desde API level 4. La voz disponible depende
  del motor TTS instalado en el dispositivo (Google TTS, Samsung TTS, etc.).
```

---

## 5. Componente: ESP32 (Control FÃ­sico)

### 5.1 Arquitectura del Firmware

```mermaid
graph TB
    subgraph "Hardware Layer"
        MOTOR[Driver Motores<br/>L298N Dual H-Bridge<br/>Gear Motor TT Yellow 5V]
        SENS_CLIFF[Sensores Cliff<br/>VL53L0X ToF x3]
        SENS_DIST_F[Sensor Distancia Frontal<br/>HC-SR04]
        SENS_DIST_R[Sensor Distancia Trasero<br/>HC-SR04]
        RGB_LED[RGB LED<br/>4 patas, 256 colores]
        BATT[Monitor BaterÃ­a<br/>Pack 3S2P 11.1V]
    end
    
    subgraph "Firmware Layer"
        MAIN[main.cpp<br/>Loop Principal]
        BT_SERVER[BLE Server]
        MOTOR_CTRL[Motor Controller<br/>2 ruedas + rueda libre]
        SENSOR_MGR[Sensor Manager<br/>DetenciÃ³n a menos de 10cm]
        LED_CTRL[LED Controller<br/>RGB LED simple]
        SAFETY[Safety Monitor<br/>ObstÃ¡culos < 10cm â†’ STOP]
    end
    
    subgraph "Communication"
        BLE[BLE UART Service]
        JSON[JSON Parser/Builder]
    end
    
    BLE --> BT_SERVER
    BT_SERVER --> JSON
    JSON --> MAIN
    
    MAIN --> MOTOR_CTRL
    MAIN --> LED_CTRL
    MAIN --> SENSOR_MGR
    
    SENSOR_MGR --> SAFETY
    SAFETY --> MOTOR_CTRL
    
    MOTOR_CTRL --> MOTOR
    LED_CTRL --> RGB_LED
    SENSOR_MGR --> SENS_CLIFF
    SENSOR_MGR --> SENS_DIST_F
    SENSOR_MGR --> SENS_DIST_R
    SENSOR_MGR --> BATT
    
    SENSOR_MGR --> JSON
    JSON --> BT_SERVER
    BT_SERVER --> BLE
```

### 5.2 Estructura del Proyecto

```
esp32-firmware/
â”œâ”€â”€ platformio.ini
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.cpp
â”‚   â”œâ”€â”€ config.h
â”‚   â”œâ”€â”€ bluetooth/
â”‚   â”‚   â”œâ”€â”€ BLEServer.cpp
â”‚   â”‚   â”œâ”€â”€ BLEServer.h
â”‚   â”‚   â”œâ”€â”€ HeartbeatMonitor.cpp    # Monitor de heartbeat con timeout 3s
â”‚   â”‚   â””â”€â”€ HeartbeatMonitor.h
â”‚   â”œâ”€â”€ motors/
â”‚   â”‚   â”œâ”€â”€ MotorController.cpp     # L298N + Gear Motor TT Yellow 5V
â”‚   â”‚   â””â”€â”€ MotorController.h
â”‚   â”œâ”€â”€ sensors/
â”‚   â”‚   â”œâ”€â”€ CliffSensor.cpp         # VL53L0X ToF x3 (precisa mediciÃ³n de distancia)
â”‚   â”‚   â”œâ”€â”€ CliffSensor.h
â”‚   â”‚   â”œâ”€â”€ DistanceSensor.cpp      # HC-SR04 x2 (frontal y trasero)
â”‚   â”‚   â”œâ”€â”€ DistanceSensor.h
â”‚   â”‚   â””â”€â”€ BatteryMonitor.cpp      # Pack 6x18650 3S2P, 11.1V nominal
â”‚   â”œâ”€â”€ leds/
â”‚   â”‚   â”œâ”€â”€ LEDController.cpp       # RGB LED simple 4 patas, 256 colores
â”‚   â”‚   â””â”€â”€ LEDController.h
â”‚   â”œâ”€â”€ safety/
â”‚   â”‚   â”œâ”€â”€ SafetyMonitor.cpp       # DetenciÃ³n automÃ¡tica a < 10cm de obstÃ¡culo
â”‚   â”‚   â””â”€â”€ SafetyMonitor.h
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ JSONParser.cpp
â”‚       â””â”€â”€ Logger.cpp
â””â”€â”€ lib/
    â”œâ”€â”€ ArduinoJson/
    â”œâ”€â”€ VL53L0X/                    # LibrerÃ­a para sensores cliff ToF
    â””â”€â”€ ESP32-BLE-Arduino/
```

### 5.3 ConfiguraciÃ³n de Hardware

#### Pinout ESP32-S3 WROOM (Freenove FNK0082)

> **Pines reservados ESP32-S3 â€” NO usar**: GPIO 0, 3, 45, 46 (strapping); GPIO 19, 20 (USB);
> GPIO 35, 36, 37 (OPI PSRAM). Los pines de cÃ¡mara (GPIO 4â€“18) son de uso libre ya que el
> ESP32-S3 no lleva mÃ³dulo de cÃ¡mara en este proyecto.

```
Motores â€” L298N Dual H-Bridge (Gear Motor TT Yellow 5V):
- Motor Izquierdo FWD:  GPIO 41  (IN1 del L298N)
- Motor Izquierdo REV:  GPIO 42  (IN2 del L298N)
- Motor Derecho FWD:    GPIO 47  (IN3 del L298N)
- Motor Derecho REV:    GPIO 48  (IN4 del L298N)
- Enable A (Izq):       GPIO 1   (ENA, PWM)
- Enable B (Der):       GPIO 2   (ENB, PWM)
  â€” ConfiguraciÃ³n fÃ­sica: 2 ruedas motrices + 1 rueda de apoyo â€”

Sensores de Cliff â€” VL53L0X ToF x3 (IÂ²C):
- SDA compartido:       GPIO 21
- SCL compartido:       GPIO 22
- XSHUT Cliff F-Izq:   GPIO 11  (reset para direcciÃ³n Ãºnica IÂ²C)
- XSHUT Cliff F-Der:   GPIO 12
- XSHUT Cliff Trasero: GPIO 13

Sensor de Distancia FRONTAL â€” HC-SR04:
- Trigger: GPIO 4
- Echo:    GPIO 5

Sensor de Distancia TRASERO â€” HC-SR04:
- Trigger: GPIO 6
- Echo:    GPIO 7

RGB LED simple (4 patas, Ã¡nodo comÃºn, 256 colores por canal â†’ ~16M colores):
- Canal R:  GPIO 38  (PWM â€” LEDC canal 0)
- Canal G:  GPIO 39  (PWM â€” LEDC canal 1)
- Canal B:  GPIO 40  (PWM â€” LEDC canal 2)
- Ãnodo (+): 3.3V  (pata larga â€” Ã¡nodo comÃºn)
  Nota: nivel LOW = LED encendido; usar ledcWrite(pin, 255 - valor)

BaterÃ­a:
- Voltaje ADC:  GPIO 8   (divisor resistivo para medir pack 3S2P, ADC1_CH7)
```

> **Nota de pines**: Todos los conflictos de la revisiÃ³n anterior han sido resueltos con esta
> asignaciÃ³n. La selecciÃ³n evita los pines reservados del ESP32-S3 (USB, PSRAM, strapping).
> Los GPIOs 38/39/40 (interfaz SD card) se usan para el RGB LED dado que no se emplea SD card.

#### Especificaciones ElÃ©ctricas

```
AlimentaciÃ³n â€” Pack 6x 18650 en 3S2P:
- ConfiguraciÃ³n: 3S2P â†’ 3 celdas en serie Ã— 2 en paralelo
- TensiÃ³n nominal: 11.1V (3.7V Ã— 3S)
- TensiÃ³n mÃ¡xima (cargado): 12.6V (4.2V Ã— 3S)
- TensiÃ³n mÃ­nima (protecciÃ³n BMS): ~9.0V (3.0V Ã— 3S)
- Capacidad total: 2 Ã— capacidad de celda (p.ej. 2 Ã— 3000mAh = 6000mAh)
- BMS: 3S 20A para Li-ion 18650 (protecciÃ³n de sobrecarga, sobredescarga, cortocircuito)

RegulaciÃ³n de voltaje â€” 2 Buck Converters:
- Buck Converter #1 (Motores): 9â€“12.6V â†’ 5.0V
    Salida: alimenta L298N y Gear Motor TT Yellow
- Buck Converter #2 (ESP32 + sensores): 9â€“12.6V â†’ 5.0V
    Entrada al pin VIN del ESP32
    Regulador interno ESP32: 5V â†’ 3.3V (para lÃ³gica + VL53L0X + HC-SR04)

Motores:
- Tipo: Gear Motor TT Yellow for Arduino Robotic Car
- TensiÃ³n de operaciÃ³n: 5V (recomendado)
- Corriente: ~200mA por motor (sin carga)
- Driver: L298N Dual H-Bridge (SOLO L298N â€” no DRV8833)
- PWM Frecuencia: 1kHz
- NÃºmero de ruedas: 2 ruedas motrices + 1 rueda de apoyo (soporte)

Consumo Total (estimado):
- ESP32:        ~100mA (BT activo)
- Motores:      ~400mA (en movimiento)
- RGB LED:      ~30mA (brillo mÃ¡ximo, 3 canales Ã— 10mA tÃ­pico)
- Sensores:     ~80mA (2Ã— HC-SR04 + 3Ã— VL53L0X)
- TOTAL pico:   ~610mA (desde Buck #2) + ~400mA (Buck #1 motores)
```

### 5.4 Protocolo BLE

#### ConfiguraciÃ³n del Servicio

```
Device Name: "RobotESP32"
Service UUID: 6E400001-B5A3-F393-E0A9-E50E24DCCA9E

CaracterÃ­sticas:
1. TX (Write): Recibe comandos desde Android
   UUID: 6E400002-B5A3-F393-E0A9-E50E24DCCA9E
   Properties: WRITE, WRITE_NO_RESPONSE
   Max Length: 512 bytes

2. RX (Notify): EnvÃ­a telemetrÃ­a a Android
   UUID: 6E400003-B5A3-F393-E0A9-E50E24DCCA9E
   Properties: NOTIFY
   Interval: Cada 1 segundo (o on-demand)
```

#### MÃ¡quina de Estados de ConexiÃ³n

```mermaid
stateDiagram-v2
    [*] --> Advertising
    Advertising --> Connected: Cliente conecta
    Connected --> Advertising: Cliente desconecta
    
    state Connected {
        [*] --> Idle
        Idle --> Executing: Comando recibido
        Executing --> Idle: Comando completo
        Executing --> Emergency: Sensor de emergencia
        Executing --> BrainOffline: Heartbeat perdido >3s
        Emergency --> Idle: Emergencia resuelta
        
        Idle --> Sending: Intervalo telemetrÃ­a
        Sending --> Idle: Datos enviados
        
        Idle --> BrainOffline: Heartbeat perdido >3s
        BrainOffline --> Idle: Heartbeat restaurado
    }
    
    state BrainOffline {
        [*] --> MotorStop: STOP inmediato
        MotorStop --> AmberPulse: LEDs Ã¡mbar pulsante
        AmberPulse --> WaitHeartbeat: Esperar heartbeat
        WaitHeartbeat --> AmberPulse: Cada 500ms
    }
    
    Connected --> [*]: Reinicio
```

#### Protocolo de Heartbeat

El dispositivo Android envÃ­a un mensaje heartbeat cada 1 segundo a travÃ©s de BLE. Si el ESP32 no recibe un heartbeat durante 3 segundos, entra automÃ¡ticamente en estado **BRAIN_OFFLINE** y ejecuta el protocolo de seguridad independiente:

```
Heartbeat Protocol:
- Frecuencia: Cada 1 segundo (Android â†’ ESP32)
- Formato: {"type": "heartbeat", "timestamp": unix_ms}
- Timeout: 3 segundos sin heartbeat â†’ BRAIN_OFFLINE
- AcciÃ³n en BRAIN_OFFLINE:
  1. STOP inmediato de todos los motores
  2. LEDs en modo Ã¡mbar pulsante (cÃ³digo visual de error)
  3. Enviar telemetrÃ­a de emergencia si BLE aÃºn conectado
  4. No aceptar nuevos comandos de movimiento
- RecuperaciÃ³n: Al recibir heartbeat vÃ¡lido â†’ volver a Idle
- PropÃ³sito: El cuerpo del robot puede salvarse a sÃ­ mismo
  si el cerebro (Android) muere, independiente de la app
```

Este mecanismo desacopla la seguridad fÃ­sica de la lÃ³gica de alto nivel de la app. Si el OS Android mata el servicio, o si la app se cuelga durante un movimiento, el robot se detiene automÃ¡ticamente y muestra visualmente que su "cerebro" estÃ¡ desconectado.

### 5.5 Control de Motores

> **ConfiguraciÃ³n fÃ­sica**: 2 ruedas motrices (izquierda + derecha, Gear Motor TT Yellow 5V vÃ­a L298N)
> + 1 rueda de apoyo delantera (rueda loca/caster). Giro realizado diferencialmente.

#### Modos de Movimiento

```
Forward (Adelante):
  - Motor Izq: PWM speed% FWD
  - Motor Der: PWM speed% FWD
  â€” 2 ruedas motrices avanzan; rueda de apoyo sigue pasivamente â€”

Backward (AtrÃ¡s):
  - Motor Izq: PWM speed% REV
  - Motor Der: PWM speed% REV
  â€” Sensor trasero HC-SR04 activo; el sistema de seguridad detiene si < 10cm â€”

Left / Rotate-Left (Izquierda / Rotar a la izquierda):
  - Motor Izq: PWM speed% REV
  - Motor Der: PWM speed% FWD
  â€” Giro diferencial sobre la rueda de apoyo â€”

Right / Rotate-Right (Derecha / Rotar a la derecha):
  - Motor Izq: PWM speed% FWD
  - Motor Der: PWM speed% REV

Stop (Parar):
  - Ambos motores: PWM 0%
  - Freno activo (ambos pines LOW)

Search-Rotate (BÃºsqueda de persona):
  - Secuencia enviada desde Android cuando el robot busca a la persona
  - Ejemplo: Rotar +90Â°, esperar, Rotar -90Â°, avanzar, retroceder
  - Comando BLE tipo: move_sequence (ver 5.4)

Move-Sequence (Secuencia de movimiento):
  - El backend calcula una secuencia de pasos con duraciÃ³n total
  - Android envÃ­a el array de steps al ESP32 vÃ­a BLE
  - ESP32 ejecuta cada step en orden y sincroniza emojis con total_duration_ms
  - Formato step: {"direction": "forward", "speed": 70, "duration_ms": 800}
```

#### Tipos de Comandos BLE (TX desde Android)

```json
// Comando simple de movimiento
{"type": "move", "direction": "forward", "speed": 70}

// Secuencia de movimientos (para bÃºsqueda o respuesta a usuario)
{
  "type": "move_sequence",
  "total_duration_ms": 2400,
  "steps": [
    {"direction": "rotate_right", "speed": 50, "duration_ms": 800},
    {"direction": "stop",         "speed": 0,  "duration_ms": 400},
    {"direction": "rotate_left",  "speed": 50, "duration_ms": 800},
    {"direction": "stop",         "speed": 0,  "duration_ms": 400}
  ]
}

// Stop inmediato
{"type": "stop"}
```

#### Sistema de Rampa (Suavizado)

```mermaid
flowchart LR
    A[Comando Move] --> B[Speed Objetivo]
    B --> C{Â¿Speed actual?}
    C --> D[Incrementar/Decrementar<br/>en pasos de 10%]
    D --> E[Aplicar PWM]
    E --> F{Â¿Objetivo alcanzado?}
    F -->|No| D
    F -->|SÃ­| G[Mantener velocidad]
```

### 5.6 Sistema de Seguridad

#### Monitor de Seguridad Continuo

```mermaid
flowchart TD
    START[Loop Principal] --> CHECK_HB[Verificar heartbeat]
    CHECK_HB --> HB_OK{Â¿Heartbeat recibido<br/>en Ãºltimos 3s?}
    
    HB_OK -->|No| BRAIN_OFFLINE[BRAIN_OFFLINE:<br/>STOP motores +<br/>LEDs Ã¡mbar pulsante]
    HB_OK -->|SÃ­| CHECK_CLIFF[Leer sensores cliff]
    
    CHECK_CLIFF --> CLIFF_OK{Â¿Cliff detectado?}
    
    CLIFF_OK -->|SÃ­| EMERGENCY[STOP INMEDIATO]
    CLIFF_OK -->|No| CHECK_DIST[Leer sensor distancia]
    
    CHECK_DIST --> DIST_OK{Â¿Distancia FRONTAL < 10cm?}
    DIST_OK -->|SÃ­ y Avanzando| EMERGENCY
    DIST_OK -->|No| CHECK_DIST_R[Leer sensor distancia TRASERO]

    CHECK_DIST_R --> DIST_R_OK{Â¿Distancia TRASERA < 10cm?}
    DIST_R_OK -->|SÃ­ y Retrocediendo| EMERGENCY
    DIST_R_OK -->|No| CHECK_BATT[Leer baterÃ­a]
    
    CHECK_BATT --> BATT_OK{Â¿BaterÃ­a < 10%?}
    BATT_OK -->|SÃ­| LOW_BATT[Notificar baterÃ­a baja]
    BATT_OK -->|No| NORMAL[OperaciÃ³n normal]
    
    EMERGENCY --> NOTIFY[Enviar telemetrÃ­a emergencia]
    NOTIFY --> WAIT[Esperar intervenciÃ³n]
    
    LOW_BATT --> NORMAL
    NORMAL --> START
    WAIT --> START
```

#### Condiciones de Emergencia

```
Prioridad CRÃTICA:
1. Cliff detectado (cualquier sensor)
   â†’ STOP inmediato + notificar

2. Distancia frontal < 10cm durante movimiento adelante
   â†’ STOP inmediato + notificar

2b. Distancia trasera < 10cm durante movimiento atrÃ¡s
   â†’ STOP inmediato + notificar

3. PÃ©rdida de heartbeat BLE > 3s (BRAIN_OFFLINE)
   â†’ STOP inmediato + LEDs Ã¡mbar pulsante
   â†’ El robot se auto-protege si el Android muere
   â†’ Feedback visual independiente de la app

4. PÃ©rdida de comunicaciÃ³n BLE completa durante movimiento
   â†’ STOP inmediato + modo seguro

Prioridad ALTA:
5. BaterÃ­a < 10%
   â†’ Notificar + limitar velocidad al 50%

6. Timeout de comando > duraciÃ³n especificada
   â†’ STOP gradual + idle
```

### 5.7 Control de LEDs

> **Hardware**: RGB LED simple de 4 patas (**Ã¡nodo comÃºn** â€” pata larga conectada a 3.3V).  
> Control por PWM con `ledcAttachChannel()` + `ledcWrite()` (API Arduino ESP32 2.x).  
> No se usa WS2812B ni FastLED â€” control directo con `ledcWrite` del ESP32-S3.  
> **Ãnodo comÃºn**: nivel LOW = LED encendido â†’ se usa `255 - valor` para lÃ³gica intuitiva (0 = apagado, 255 = mÃ¡ximo brillo).

#### Modos de IluminaciÃ³n

```
Estado IDLE:
  - Color: Azul suave (R=0, G=80, B=200)
  - PatrÃ³n: RespiraciÃ³n (PWM fade in/out en los 3 canales)
  - Velocidad: Lenta (ciclo ~3s)

Estado MOVING:
  - Color: Verde (R=0, G=255, B=0)
  - PatrÃ³n: SÃ³lido
  - Intensidad: 80%

Estado ERROR:
  - Color: Rojo (R=255, G=0, B=0)
  - PatrÃ³n: Parpadeo rÃ¡pido
  - Frecuencia: 2 Hz

Estado BRAIN_OFFLINE (Heartbeat perdido):
  - Color: Ãmbar (R=255, G=160, B=0)
  - PatrÃ³n: Pulso suave (fade in/out)
  - Frecuencia: 1 Hz
  - PropÃ³sito: CÃ³digo visual independiente de la app

Estado LOW BATTERY:
  - Color: Naranja (R=255, G=100, B=0)
  - PatrÃ³n: Parpadeo lento
  - Frecuencia: 0.5 Hz

Modo Custom (comando BLE):
  - Color: RGB personalizado (valores 0-255 por canal)
  - PatrÃ³n: SÃ³lido | Blink | Breathe
  - No hay efecto Rainbow (limitaciÃ³n del LED simple)
```

#### ImplementaciÃ³n de Efectos (ledcWrite PWM)

```mermaid
flowchart TD
    A[Comando LED recibido] --> B{Tipo de patrÃ³n}
    
    B -->|Solid| C[ledcWrite R/G/B con valores fijos]
    B -->|Blink| D[Toggle ON/OFF cada 500ms<br/>ledcWrite valor â†” 0]
    B -->|Breathe| E[Fade PWM 0â†’255â†’0<br/>en los 3 canales proporcionalmente]
    
    C --> F[ESP32 aplica PWM directo]
    D --> F
    E --> F
```

```cpp
// InicializaciÃ³n â€” asociar pines a canales LEDC (1 kHz, 8 bits, como ejemplo Freenove)
void setupLED() {
  ledcAttachChannel(LED_PIN_R, LED_PWM_FREQ, 8, LED_PWM_CHANNEL_R);
  ledcAttachChannel(LED_PIN_G, LED_PWM_FREQ, 8, LED_PWM_CHANNEL_G);
  ledcAttachChannel(LED_PIN_B, LED_PWM_FREQ, 8, LED_PWM_CHANNEL_B);
}

// Control de color â€” Ã¡nodo comÃºn: nivel bajo = encendido â†’ invertir valores
void setLED(uint8_t r, uint8_t g, uint8_t b) {
  ledcWrite(LED_PIN_R, 255 - r);  // 0 = apagado, 255 = brillo mÃ¡ximo (lÃ³gica usuario)
  ledcWrite(LED_PIN_G, 255 - g);
  ledcWrite(LED_PIN_B, 255 - b);
}
```

### 5.8 TelemetrÃ­a y Logging

#### Datos de TelemetrÃ­a

```json
{
  "type": "telemetry",
  "timestamp": 1234567890,
  "battery": {
    "voltage": 7.2,
    "percentage": 75,
    "charging": false
  },
  "sensors": {
    "cliff_front_left": false,
    "cliff_front_right": false,
    "cliff_rear": false,
    "distance_front": 150,
    "light_level": 300
  },
  "motors": {
    "left_speed": 50,
    "right_speed": 50,
    "direction": "forward"
  },
  "leds": {
    "mode": "moving",
    "color": [0, 255, 0]
  },
  "uptime": 3600,
  "free_heap": 120000,
  "heartbeat": {
    "last_received_ms": 1234567890,
    "brain_online": true
  }
}
```

#### EnvÃ­o de TelemetrÃ­a

```
Modo periÃ³dico: Cada 1 segundo (si conectado)
Modo on-demand: Al recibir comando "telemetry"
Modo emergencia: Inmediato al detectar condiciÃ³n crÃ­tica
```

### 5.9 ConfiguraciÃ³n del Firmware

```cpp
// config.h

// Bluetooth
#define BLE_DEVICE_NAME "RobotESP32"
#define BLE_MTU_SIZE 512

// Motores
#define MOTOR_PWM_FREQ 1000
#define MOTOR_PWM_RESOLUTION 8
#define MOTOR_RAMP_STEP 10
#define MOTOR_RAMP_DELAY_MS 50

// Seguridad
#define CLIFF_CHECK_INTERVAL_MS 100
#define DISTANCE_THRESHOLD_CM 10         // ObstÃ¡culo frontal o trasero < 10cm â†’ STOP
#define LOW_BATTERY_THRESHOLD 10
#define BLE_TIMEOUT_MS 10000
#define HEARTBEAT_TIMEOUT_MS 3000        // 3s sin heartbeat = BRAIN_OFFLINE
#define HEARTBEAT_EXPECTED_INTERVAL 1000 // Esperar heartbeat cada 1s

// Motores â€” Gear Motor TT Yellow 5V vÃ­a L298N
#define MOTOR_IN1  41                    // Motor Izquierdo FWD
#define MOTOR_IN2  42                    // Motor Izquierdo REV
#define MOTOR_IN3  47                    // Motor Derecho FWD
#define MOTOR_IN4  48                    // Motor Derecho REV
#define MOTOR_ENA  1                     // Enable A (Izq) â€” PWM
#define MOTOR_ENB  2                     // Enable B (Der) â€” PWM

// Sensores Cliff â€” VL53L0X ToF x3 (IÂ²C)
#define I2C_SDA         21
#define I2C_SCL         22
#define XSHUT_CLIFF_FL  11               // Cliff Front-Left
#define XSHUT_CLIFF_FR  12               // Cliff Front-Right
#define XSHUT_CLIFF_RR  13               // Cliff Rear

// Sensores HC-SR04 (frontal y trasero)
#define ULTRASONIC_TIMEOUT_US 30000
#define DIST_TRIG_FRONT 4
#define DIST_ECHO_FRONT 5
#define DIST_TRIG_REAR  6
#define DIST_ECHO_REAR  7

// BaterÃ­a â€” Pack 6x 18650 3S2P
#define BATTERY_ADC_PIN  8               // ADC1_CH7 â€” divisor resistivo 3S2P
#define BATTERY_ADC_SAMPLES 10
#define BATTERY_VOLTAGE_MIN 9.0          // ~3.0V Ã— 3S (BMS protege antes)
#define BATTERY_VOLTAGE_MAX 12.6         // 4.2V Ã— 3S (carga completa)

// Motor â€” Gear Motor TT Yellow 5V via L298N
#define MOTOR_VOLTAGE_TARGET 5.0         // Buck Converter #1 â†’ 5V

// RGB LED Ã¡nodo comÃºn, 4 patas (GPIO 38/39/40 â€” sin WS2812B / sin FastLED)
// Ãnodo comÃºn: nivel LOW = encendido â†’ setLED usa (255 - valor)
#define LED_PIN_R       38               // Canal rojo
#define LED_PIN_G       39               // Canal verde
#define LED_PIN_B       40               // Canal azul
#define LED_PWM_FREQ    1000             // 1 kHz (igual que ejemplo Freenove)
#define LED_PWM_CHANNEL_R 0
#define LED_PWM_CHANNEL_G 1
#define LED_PWM_CHANNEL_B 2
#define LED_BRIGHTNESS_MAX 255

// TelemetrÃ­a
#define TELEMETRY_INTERVAL_MS 1000
```

---

## 6. Protocolos de ComunicaciÃ³n

### 6.1 Android â†” Backend (WebSocket Streaming + REST Auxiliar)

#### EspecificaciÃ³n del Protocolo Principal (WebSocket)

```
Protocolo: WebSocket sobre TLS (wss://)
Formato: JSON (mensajes de control) + Binary (audio del usuario)
Encoding: UTF-8 (JSON), Raw bytes (audio)
AutenticaciÃ³n: API Key en primer mensaje (handshake)
Certificate Pinning: Obligatorio en cliente Android

Mensajes del cliente:
- auth: AutenticaciÃ³n con API Key
- interaction_start: Inicio de interacciÃ³n con contexto
- binary: Audio grabado del usuario (formato AAC/WebM)
- audio_end: Fin de grabaciÃ³n
- text: Texto directo (alternativa a audio)
- image: Imagen en base64 (registro o contexto visual)
- video: Video en base64 (contexto visual cuando se solicita)

Mensajes del servidor:
- auth_ok: ConfirmaciÃ³n de autenticaciÃ³n
- emotion: Tag de emociÃ³n del LLM (enviado primero)
- text_chunk: Fragmento de texto de respuesta (streaming progresivo desde Gemini)
- capture_request: Solicitud de captura de foto o video al cliente
- response_meta: Metadata (emojis, acciones)
- stream_end: Fin de streaming
- error: Error con cÃ³digo y mensaje

Keepalive:
- Ping/Pong: Cada 30 segundos
- Timeout: 10 segundos sin pong = reconectar
```

#### Protocolo REST Auxiliar (HTTPS)

```
Protocolo: HTTPS/1.1 (TLS obligatorio)
Formato: JSON
Encoding: UTF-8
AutenticaciÃ³n: API Key en header X-API-Key
Certificate Pinning: Obligatorio

Uso: Operaciones de gestiÃ³n que no requieren streaming
```

#### Endpoints REST Auxiliares

```
GET /api/health
  â†’ Health check del backend

POST /api/users/{user_id}/memory
  â†’ Agregar memoria manualmente

GET /api/users/{user_id}/memory
  â†’ Consultar memoria del usuario

POST /api/face/register
  â†’ Registrar nuevo rostro
```

#### Formato de Errores Estandarizado

```json
{
  "error": true,
  "error_code": "ERROR_CODE_SNAKE_CASE",
  "message": "Mensaje legible para humanos",
  "details": "InformaciÃ³n tÃ©cnica adicional",
  "recoverable": true|false,
  "retry_after": 5,
  "timestamp": "2026-02-08T10:30:00Z",
  "request_id": "uuid-v4"
}
```

### 6.2 Android â†” ESP32 (Bluetooth LE)

#### EspecificaciÃ³n del Protocolo

```
Transport: Bluetooth Low Energy 4.0+
Profile: Custom (Nordic UART Service)
MTU: 512 bytes
Formato: JSON UTF-8
Frecuencia: On-demand + telemetrÃ­a periÃ³dica 1Hz + heartbeat 1Hz
```

#### Estructura de Comandos

```json
{
  "id": "uuid-v4",
  "type": "move|light|telemetry|stop|heartbeat",
  "params": {
    // ParÃ¡metros especÃ­ficos del comando
  },
  "timestamp": 1234567890
}
```

#### Estructura de Respuestas

```json
{
  "id": "uuid-v4-del-comando",
  "status": "ok|error|warning",
  "data": {
    // Datos de respuesta
  },
  "error_msg": "descripciÃ³n si status=error",
  "timestamp": 1234567890
}
```

#### Manejo de DesconexiÃ³n

```mermaid
sequenceDiagram
    participant A as Android
    participant E as ESP32
    
    Note over A,E: ConexiÃ³n establecida
    
    A->>E: Comando
    E-->>A: ACK
    
    Note over A,E: ConexiÃ³n perdida
    
    loop ReconexiÃ³n
        A->>E: Intento de conexiÃ³n
        Note over A: Espera 2s
    end
    
    E-->>A: ConexiÃ³n restaurada
    A->>E: Solicitar estado actual
    E-->>A: TelemetrÃ­a completa
    
    Note over A: Sincronizar estado UI
```

### 6.3 CÃ³digos de Error Globales

```
Rango 1xxx: Errores de AutenticaciÃ³n
- 1001: API_KEY_MISSING
- 1002: API_KEY_INVALID
- 1003: API_KEY_EXPIRED

Rango 2xxx: Errores de ValidaciÃ³n
- 2001: INVALID_REQUEST_FORMAT
- 2002: MISSING_REQUIRED_FIELD
- 2003: FILE_TOO_LARGE
- 2004: UNSUPPORTED_FILE_FORMAT

Rango 3xxx: Errores de Servicios Externos
- 3001: GEMINI_SERVICE_UNAVAILABLE
- 3002: GEMINI_RATE_LIMIT_EXCEEDED

Rango 4xxx: Errores de Datos
- 4001: USER_NOT_FOUND
- 4002: FACE_NOT_RECOGNIZED
- 4003: MEMORY_NOT_FOUND
- 4004: DATABASE_ERROR

Rango 5xxx: Errores de Sistema
- 5001: INTERNAL_SERVER_ERROR
- 5002: OUT_OF_MEMORY
- 5003: DISK_FULL
- 5004: CONFIGURATION_ERROR

Rango 6xxx: Errores de Bluetooth
- 6001: BLUETOOTH_NOT_ENABLED
- 6002: DEVICE_NOT_FOUND
- 6003: CONNECTION_FAILED
- 6004: COMMAND_TIMEOUT
- 6005: CHARACTERISTIC_NOT_FOUND

Rango 7xxx: Errores de Hardware (ESP32)
- 7001: CLIFF_DETECTED
- 7002: OBSTACLE_DETECTED
- 7003: LOW_BATTERY
- 7004: MOTOR_FAILURE
- 7005: SENSOR_MALFUNCTION
- 7006: HEARTBEAT_LOST (BRAIN_OFFLINE)

Rango 8xxx: Errores de WebSocket
- 8001: WS_AUTH_FAILED
- 8002: WS_CONNECTION_LOST
- 8003: WS_MESSAGE_TOO_LARGE
- 8004: WS_INVALID_MESSAGE_FORMAT
- 8005: WS_STREAM_INTERRUPTED
```

---

## 7. GestiÃ³n de Estados del Sistema

### 7.1 Estados Globales del Robot

```mermaid
stateDiagram-v2
    [*] --> BOOT
    BOOT --> IDLE: InicializaciÃ³n OK
    BOOT --> ERROR: Fallo inicializaciÃ³n
    
    IDLE --> LISTENING: Wake Word [transiciÃ³n visual inmediata]
    LISTENING --> SEARCHING: Audio capturado + CÃ¡mara activada
    SEARCHING --> GREETING: Rostro reconocido (score > 0.70)
    SEARCHING --> REGISTERING: Rostro desconocido
    SEARCHING --> IDLE: Timeout 5s sin rostro detectado
    GREETING --> IDLE: Saludo completado
    REGISTERING --> LISTENING: Robot pregunta nombre
    LISTENING --> THINKING: Nombre recibido
    THINKING --> RESPONDING: Emotion tag recibido vÃ­a WS
    THINKING --> ERROR: Timeout/Error
    RESPONDING --> IDLE: Stream completo
    
    state RESPONDING {
        [*] --> EmotionUpdate
        EmotionUpdate --> StreamingAudio
        StreamingAudio --> ShowingEmojis
        ShowingEmojis --> ExecutingActions
        ExecutingActions --> [*]
    }
    
    IDLE --> MOVING: Comando movimiento
    MOVING --> IDLE: Movimiento completo
    MOVING --> EMERGENCY: Sensor crÃ­tico
    MOVING --> EMERGENCY: Heartbeat perdido (BRAIN_OFFLINE)
    
    ERROR --> IDLE: Retry exitoso
    EMERGENCY --> IDLE: Usuario interviene
    
    ERROR --> [*]: Error crÃ­tico
    EMERGENCY --> [*]: Apagado forzado
```

### 7.2 Matriz de Transiciones Permitidas

| Estado Actual | TransiciÃ³n Permitida A | Trigger |
|---------------|------------------------|---------|
| BOOT | IDLE | Sistema inicializado |
| BOOT | ERROR | Fallo en inicializaciÃ³n |
| IDLE | LISTENING | Wake word "Hey Robi" detectado (transiciÃ³n visual inmediata) |
| IDLE | MOVING | Comando de movimiento |
| LISTENING | SEARCHING | CÃ¡mara activada + audio capturado |
| SEARCHING | GREETING | Rostro reconocido (ML Kit + FaceNet, score > 0.70) |
| SEARCHING | REGISTERING | Rostro desconocido o sin match |
| SEARCHING | IDLE | Timeout 5s sin rostro detectado |
| SEARCHING | ERROR | Error de cÃ¡mara |
| GREETING | IDLE | Saludo con nombre completado |
| REGISTERING | LISTENING | Robot pregunta nombre |
| LISTENING | THINKING | Nombre recibido vÃ­a audio |
| LISTENING | ERROR | Timeout 10s sin audio |
| THINKING | RESPONDING | Backend envÃ­a emotion tag vÃ­a WS |
| THINKING | ERROR | WebSocket error/timeout |
| RESPONDING | IDLE | Stream completo |
| MOVING | IDLE | DuraciÃ³n completa |
| MOVING | EMERGENCY | Cliff/obstÃ¡culo detectado |
| MOVING | EMERGENCY | Heartbeat perdido (BRAIN_OFFLINE en ESP32) |
| ERROR | IDLE | Reintentar exitoso |
| EMERGENCY | IDLE | Usuario resuelve/reset |

### 7.3 SincronizaciÃ³n de Estados entre Componentes

```mermaid
sequenceDiagram
    participant A as Android UI
    participant S as StateManager
    participant FR as FaceRecognition
    participant B as Backend (WebSocket)
    participant E as ESP32
    
    Note over A: Usuario dice "Hey Robi"
    A->>S: setState(LISTENING) [INMEDIATO]
    S->>A: Update UI (ğŸ‘‚)
    S->>E: Notificar estado (via BLE)
    E->>E: LED azul pulsante
    
    A->>S: setState(SEARCHING)
    S->>A: Update UI (ğŸ”)
    A->>FR: Activar cÃ¡mara, detectar rostro
    FR-->>A: Rostro encontrado + user_id
    
    Note over A: Saludo o registro
    A->>S: setState(GREETING) o setState(REGISTERING)
    S->>A: Update UI (ğŸ‘‹ o â“)
    A->>B: WS: interaction_start + user_id + face_confidence
    
    B-->>A: WS: emotion tag [greeting/curious]
    A->>S: setState(EMOTION)
    S->>A: Update UI (emoji greeting)
    
    B-->>A: WS: text_chunks en streaming (saludo)
    A->>A: Android TTS reproduce saludo inmediatamente
    
    B-->>A: WS: stream_end
    A->>S: setState(IDLE)
    S->>A: Update UI (ğŸ¤–)
    S->>E: Notificar estado
    E->>E: LED azul respiraciÃ³n
```

### 7.4 Persistencia de Estado

```
Estado guardado al cerrar app:
- Ãšltimo user_id reconocido
- Ãšltimo estado antes de cerrar
- Dispositivo Bluetooth conectado (MAC)
- ConfiguraciÃ³n de sensibilidad wake word

Estado recuperado al abrir app:
- Reconectar Bluetooth si es posible
- Volver a IDLE (siempre inicio seguro)
- Restaurar configuraciÃ³n usuario
```

---

## 8. Seguridad y Privacidad

### 8.1 Modelo de Amenazas

#### Amenazas Identificadas

1. **Acceso no autorizado al backend**
   - Riesgo: Medio
   - MitigaciÃ³n: API Key, HTTPS obligatorio, certificate pinning

2. **IntercepciÃ³n de comunicaciÃ³n Android â†” Backend**
   - Riesgo: **Alto** (la red local no es segura por defecto)
   - MitigaciÃ³n: **HTTPS obligatorio + certificate pinning**
   - JustificaciÃ³n: La red local puede contener dispositivos IoT comprometidos,
     equipos con malware, o redes de invitados. Enviar grabaciones de audio
     y embeddings faciales sin cifrar permite a cualquier atacante en la misma
     WiFi interceptarlos con herramientas como Wireshark.

3. **Acceso fÃ­sico no autorizado al robot**
   - Riesgo: Bajo (uso domÃ©stico)
   - MitigaciÃ³n: No aplicable (confianza familiar)

4. **Fuga de datos biomÃ©tricos (rostros)**
   - Riesgo: Medio
   - MitigaciÃ³n: Embeddings solo (no imÃ¡genes), HTTPS en trÃ¡nsito

5. **Grabaciones de audio almacenadas**
   - Riesgo: Medio
   - MitigaciÃ³n: Limpieza automÃ¡tica despuÃ©s de 24h, HTTPS en trÃ¡nsito

6. **Ataque man-in-the-middle dentro de la red local**
   - Riesgo: Medio
   - MitigaciÃ³n: Certificate pinning impide que un atacante suplante al servidor
     aunque envenene DNS o ARP dentro de la red local

### 8.2 ImplementaciÃ³n de Seguridad

#### AutenticaciÃ³n API Key

```
GeneraciÃ³n:
- Algoritmo: secrets.token_urlsafe(32)
- Longitud: 43 caracteres
- Formato: base64url
- RotaciÃ³n: Manual (recomendado cada 6 meses)

Almacenamiento Android:
- EncryptedSharedPreferences (Android Keystore)
- Nunca en texto plano
- No en cÃ³digo fuente

ValidaciÃ³n Backend:
- Middleware de FastAPI
- ComparaciÃ³n constant-time (evitar timing attacks)
- Rate limiting: 100 requests/minuto por API Key
```

#### EncriptaciÃ³n de Datos Sensibles

```
Embeddings Faciales:
- Almacenamiento: BLOB en SQLite
- Sin encriptaciÃ³n adicional (ya son embeddings, no imÃ¡genes)
- Acceso: Solo mediante API autenticada

Audio Temporal:
- RetenciÃ³n: 24 horas mÃ¡ximo
- Limpieza: Cron job diario
- Nombres: UUID aleatorios
- Sin metadatos de usuario en filesystem

Memoria de Usuario:
- Almacenamiento: SQLite con journal_mode=WAL
- Backups: Opcionales, encriptados con gpg
```

### 8.3 Red y ComunicaciÃ³n

#### ConfiguraciÃ³n de Red Segura

```
Backend:
- Bind a 0.0.0.0:9393 (accesible en LAN, Nginx escucha en 9393)
- HTTPS obligatorio (TLS 1.2+, gestionado por Nginx con Docker Compose)
- Firewall: Solo permitir puerto 9393 en red local
- IP permitidas: 192.168.2.0/24 (configurar segÃºn red)
- Certificado: Autofirmado con rotaciÃ³n anual

Android:
- URL base: https://192.168.2.200:9393 (HTTPS obligatorio, vÃ­a Nginx)
- Certificate pinning: Fingerprint del certificado del servidor
  hardcodeado en la app
- Sin exposiciÃ³n a internet
- WiFi: WPA2/WPA3 personal

ESP32:
- Bluetooth: Emparejamiento con PIN
- Sin WiFi (solo BLE point-to-point)
```

#### HTTPS con Certificado Autofirmado (OBLIGATORIO)

```bash
# Generar certificado para LAN (rotaciÃ³n recomendada cada 12 meses)
openssl req -x509 -newkey rsa:4096 -nodes \
  -keyout certs/key.pem -out certs/cert.pem -days 365 \
  -subj "/CN=192.168.2.200"

# Obtener fingerprint para certificate pinning en Android
openssl x509 -in certs/cert.pem -pubkey -noout | \
  openssl pkey -pubin -outform der | \
  openssl dgst -sha256 -binary | \
  openssl enc -base64

# Configurar FastAPI con TLS
uvicorn main:app --host 0.0.0.0 --port 8000 \
  --ssl-keyfile certs/key.pem --ssl-certfile certs/cert.pem
```

Alternativa para desarrollo: usar Nginx como reverse proxy TLS frente a la aplicaciÃ³n Python, de modo que la lÃ³gica de la app no necesite cambios.

```
# Nginx reverse proxy (alternativa)
server {
    listen 8000 ssl;
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    location / {
        proxy_pass http://127.0.0.1:8080;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }
}
```

#### Certificate Pinning en Android

El certificate pinning hardcodea el fingerprint Ãºnico del certificado del servidor en la app Android. La app rechazarÃ¡ conectarse a cualquier servidor que no presente exactamente esa identidad digital, incluso dentro de la red local.

```
ImplementaciÃ³n: OkHttp CertificatePinner
Algoritmo: SHA-256 del certificado pÃºblico
Efecto: Previene ataques man-in-the-middle incluso si un
        atacante envenena DNS o ARP en la red local
ActualizaciÃ³n: Al rotar certificado del servidor, actualizar
              el fingerprint en la app y redistribuir APK

ConfiguraciÃ³n en OkHttp:
  CertificatePinner.Builder()
    .add("192.168.2.200", "sha256/<fingerprint-base64>")
    .build()

red_segura_config.xml (Android):
  <network-security-config>
    <domain-config>
      <domain includeSubdomains="false">192.168.2.200</domain>
      <pin-set>
        <pin digest="SHA-256">BASE64_FINGERPRINT</pin>
      </pin-set>
    </domain-config>
  </network-security-config>
```

### 8.4 Privacidad de Datos

#### Principios Aplicados

1. **MinimizaciÃ³n de datos**
   - Solo recopilar lo estrictamente necesario
   - Audio/video temporal (24h mÃ¡ximo)

2. **Transparencia**
   - Usuario sabe quÃ© se graba y procesa
   - Logs accesibles en la app

3. **Control del usuario**
   - BotÃ³n para eliminar toda su memoria
   - Desactivar reconocimiento facial

4. **Almacenamiento local**
   - Sin servicios cloud externos
   - Datos permanecen en red local

#### RetenciÃ³n de Datos

```
Audio de entrada: 24 horas â†’ EliminaciÃ³n automÃ¡tica
Video/ImÃ¡genes de contexto: 1 hora â†’ EliminaciÃ³n automÃ¡tica
Memoria usuario: Indefinido (hasta borrado manual)
Embeddings faciales: Indefinido (hasta borrado manual)
Logs sistema: 30 dÃ­as â†’ RotaciÃ³n
```

> Nota: No se genera ni almacena audio de respuesta en el backend.

### 8.5 Seguridad del ESP32

#### ProtecciÃ³n de Firmware

```
Bluetooth:
- Emparejamiento: Requerido con PIN (1234)
- Bonding: Habilitado (recordar dispositivo)
- Solo un dispositivo conectado a la vez

FÃ­sico:
- BotÃ³n de reset fÃ­sico
- No exponer pines de programaciÃ³n al exterior
- Carcasa cerrada con tornillos
```

#### Comandos de Emergencia

```
Android puede enviar:
- "EMERGENCY_STOP": Detiene todo inmediatamente
- "RESET": Reinicia ESP32 (software reset)
- "SAFE_MODE": Deshabilita movimiento, solo LEDs

ESP32 puede iniciar:
- Auto-stop si pierde heartbeat por > 3s (BRAIN_OFFLINE)
- LEDs Ã¡mbar pulsante como cÃ³digo visual de "cerebro desconectado"
- Emergency stop si detecta cliff
- Safe mode si baterÃ­a < 5%
- Auto-stop si pierde conexiÃ³n BLE completa durante movimiento
```

---

## 9. Manejo de Errores y RecuperaciÃ³n

### 9.1 Estrategia General

```mermaid
flowchart TD
    ERROR[Error Detectado] --> CLASSIFY{Clasificar Error}
    
    CLASSIFY -->|Transitorio| RETRY[Reintentar con backoff]
    CLASSIFY -->|Permanente| FALLBACK[Modo fallback]
    CLASSIFY -->|CrÃ­tico| SAFE[Modo seguro]
    
    RETRY --> COUNT{Â¿Intentos < 3?}
    COUNT -->|SÃ­| WAIT[Esperar 2^n segundos]
    COUNT -->|No| FALLBACK
    WAIT --> RETRY
    
    FALLBACK --> NOTIFY[Notificar usuario]
    NOTIFY --> CONTINUE[Continuar con funcionalidad reducida]
    
    SAFE --> STOP[Detener operaciÃ³n peligrosa]
    STOP --> ALERT[Alerta al usuario]
    ALERT --> MANUAL[Esperar intervenciÃ³n manual]
```

### 9.2 Errores por Componente

#### Backend

```
Timeout Gemini:
  Tipo: Transitorio
  AcciÃ³n: Reintentar 3 veces con backoff exponencial
  Fallback: Respuesta predefinida "Lo siento, no puedo procesar ahora"
  
Error de Base de Datos:
  Tipo: Permanente
  AcciÃ³n: Log error, modo read-only
  Fallback: Funcionalidad sin memoria
  
Out of Memory:
  Tipo: CrÃ­tico
  AcciÃ³n: Limpiar archivos temporales, reiniciar proceso
  
Face Recognition falla:
  Tipo: Transitorio
  AcciÃ³n: Solicitar mejor iluminaciÃ³n
  Fallback: Continuar sin identificaciÃ³n de usuario
```

#### Android

```
Backend no responde (WebSocket desconectado):
  Tipo: Transitorio
  AcciÃ³n: Reconexion WebSocket automÃ¡tica con backoff exponencial
  Fallback: Mostrar "Backend no disponible" + cara desconectada (ğŸ”Œ)
  
Bluetooth desconectado:
  Tipo: Transitorio
  AcciÃ³n: Intentar reconexiÃ³n automÃ¡tica cada 5s
  Fallback: Funcionalidad sin movimiento fÃ­sico
  Nota: ESP32 entra en BRAIN_OFFLINE automÃ¡ticamente
        por pÃ©rdida de heartbeat
  
Wake Word no detecta:
  Tipo: ConfiguraciÃ³n
  AcciÃ³n: Ajustar sensibilidad
  Fallback: BotÃ³n manual para activar
  
Sin permisos:
  Tipo: ConfiguraciÃ³n
  AcciÃ³n: Mostrar diÃ¡logo explicativo + abrir Settings

Servicio matado por Android OS:
  Tipo: Transitorio
  AcciÃ³n: ServiceWatchdog (AlarmManager) detecta y reinicia
  Nota: ESP32 se protege vÃ­a heartbeat timeout
  
Android TTS no disponible:
  Tipo: ConfiguraciÃ³n
  AcciÃ³n: Solicitar instalaciÃ³n del motor TTS del sistema
  Fallback: Mostrar texto de respuesta en pantalla (modo silencioso)
  
Error al capturar foto/video:
  Tipo: Transitorio
  AcciÃ³n: Reintentar captura una vez
  Fallback: Continuar interacciÃ³n sin adjunto visual
  
Out of Storage:
  Tipo: Permanente
  AcciÃ³n: Limpiar cache de emojis
  Fallback: Descargar emojis on-demand
```

#### ESP32

```
Cliff detectado:
  Tipo: CrÃ­tico
  AcciÃ³n: STOP inmediato + notificar + LED rojo parpadeante
  RecuperaciÃ³n: Manual (usuario mueve robot)
  
Heartbeat perdido (BRAIN_OFFLINE):
  Tipo: CrÃ­tico
  AcciÃ³n: STOP inmediato + LEDs Ã¡mbar pulsante
  RecuperaciÃ³n: AutomÃ¡tica al recibir heartbeat vÃ¡lido
  Nota: El robot se auto-protege independientemente de la app

BLE desconectado completamente durante movimiento:
  Tipo: CrÃ­tico
  AcciÃ³n: STOP inmediato + modo seguro
  RecuperaciÃ³n: AutomÃ¡tica al reconectar
  
Motor no responde:
  Tipo: Permanente
  AcciÃ³n: Deshabilitar motor afectado + notificar
  Fallback: Movimiento limitado con un motor
  
Sensor falla:
  Tipo: Permanente
  AcciÃ³n: Deshabilitar sensor + aumentar precauciÃ³n
  Fallback: Velocidad reducida al 50%
  
BaterÃ­a crÃ­tica (<5%):
  Tipo: CrÃ­tico
  AcciÃ³n: STOP gradual + LED naranja fijo + notificar
  RecuperaciÃ³n: Cargar baterÃ­a
```

### 9.3 Logs y DiagnÃ³stico

#### Sistema de Logging

```
Backend (Python):
- LibrerÃ­a: structlog
- Formato: JSON
- Niveles: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Salida: Archivo rotativo + stdout
- RotaciÃ³n: Diaria, mantener 30 dÃ­as

Android (Kotlin):
- LibrerÃ­a: Timber
- Formato: Texto estructurado
- Niveles: VERBOSE, DEBUG, INFO, WARN, ERROR
- Salida: Logcat + archivo (solo ERROR+)
- Persistencia: 7 dÃ­as

ESP32 (C++):
- LibrerÃ­a: Serial (solo Serial â€” SD card no disponible)
- Formato: Texto simple
- Niveles: INFO, WARNING, ERROR
- Salida: Serial USB (115200 baud)
- Persistencia: Solo en sesiÃ³n (RAM volÃ¡til)
- Nota: GPIO 38/39/40 (SDMMC del ESP32-S3) estÃ¡n ocupados por el RGB LED;
         la tarjeta SD es fÃ­sicamente incompatible con esta asignaciÃ³n de pines.
```

#### Estructura de Logs

```json
// Backend
{
  "timestamp": "2026-02-08T10:30:15.123Z",
  "level": "ERROR",
  "logger": "services.gemini",
  "message": "Gemini API timeout",
  "request_id": "uuid-v4",
  "user_id": "user_juan_123",
  "duration_ms": 30001,
  "retry_count": 2,
  "error_code": "GEMINI_TIMEOUT"
}
```

### 9.4 Monitoreo de Salud

```mermaid
flowchart LR
    A[Health Check cada 30s] --> B{WebSocket conectado?}
    B -->|SÃ­| C[Estado OK]
    B -->|No| D[Intentar reconexiÃ³n WSS]
    
    C --> E[Verificar ESP32]
    E --> F{BLE conectado?}
    F -->|SÃ­| G[Solicitar telemetrÃ­a]
    F -->|No| H[Intentar reconexiÃ³n]
    
    G --> I{BaterÃ­a OK?}
    I -->|SÃ­ >15%| J[Todo OK]
    I -->|No â‰¤15%| K[Advertencia baterÃ­a â€” indicador rojo parpadeante]
    
    G --> O{Heartbeat OK?}
    O -->|SÃ­| J
    O -->|No| P[ESP32 en BRAIN_OFFLINE]
    
    D --> L[Mostrar banner offline]
    H --> M[Mostrar banner BLE offline]
    K --> N[Mostrar banner baterÃ­a]
    P --> Q[Reiniciar heartbeat]
```

---

## 10. Requisitos de Hardware y Software

### 10.1 Backend

#### Hardware MÃ­nimo

```
Procesador: Intel i3 o equivalente (2 cores)
RAM: 4GB (8GB recomendado)
Almacenamiento: 10GB disponibles
Red: Ethernet/WiFi con IP estÃ¡tica

Opciones de Despliegue:
- Laptop/Desktop en red local
- Raspberry Pi 4 (4GB RAM mÃ­nimo)
- Mini PC (Intel NUC o similar)
```

#### Software

```
Sistema Operativo: 
- Ubuntu 22.04 LTS o superior
- Windows 10/11 (WSL2 recomendado)
- macOS 12+

Python: 3.11 o superior
pip: Ãšltima versiÃ³n

Dependencias Python principales:
- fastapi, uvicorn[standard]
- deepagents                   # LangChain Deep Agents SDK (agent harness)
- langchain-google-genai       # Adapter LangChain para Gemini Flash Lite
- langgraph                    # Runtime del agente (streaming, persistencia, human-in-loop)
- google-generativeai          # Gemini SDK base
- sqlalchemy, aiosqlite        # Base de datos SQLite async
- python-dotenv, pydantic      # ConfiguraciÃ³n y validaciÃ³n
- structlog                    # Logging estructurado
- streamlit                    # Simulador de pruebas (tests/streamlit_simulator/)

Dependencias de despliegue:
- Docker + Docker Compose      # Backend y Nginx en contenedores
- Nginx                        # Reverse proxy TLS (dentro de Docker Compose)
```

### 10.2 Android

#### Hardware MÃ­nimo

```
Dispositivo: Smartphone/Tablet Android
VersiÃ³n Android: 7.0 (API 24) o superior
RAM: 2GB mÃ­nimo
Almacenamiento: 100MB disponibles
Pantalla: 5" mÃ­nimo (1280x720)

CaracterÃ­sticas requeridas:
- MicrÃ³fono
- CÃ¡mara FRONTAL (cÃ¡mara trasera no se usa)
- Bluetooth 4.0+
- WiFi
- Modo landscape fijo (no se requiere acelerÃ³metro)
```

#### Software

```
Android OS: 7.0 - 13.0 (compatible)
Google Play Services: Ãšltima versiÃ³n (para ML Kit)

Permisos runtime requeridos:
- RECORD_AUDIO
- CAMERA
- BLUETOOTH
- BLUETOOTH_CONNECT
- BLUETOOTH_SCAN
```

### 10.3 ESP32

#### Hardware

```
Microcontrolador: ESP32-WROOM-32
Flash: 4MB mÃ­nimo
RAM: 520KB (incluido en chip)

Componentes adicionales:
- Driver motores: L298N Dual H-Bridge (SOLO L298N â€” no DRV8833)
- Motores: 2x Gear Motor TT Yellow for Arduino Robotic Car, 5V + 1 rueda de apoyo
- Sensores cliff: 3x VL53L0X ToF (distancia precisa para detecciÃ³n de caÃ­das)
- Sensores distancia: 2x HC-SR04 ultrasÃ³nico (frontal + trasero)
- LED: RGB LED simple 4 patas (cÃ¡todo/Ã¡nodo comÃºn), 256 colores por canal
- BaterÃ­a: 6x 18650 Li-ion en configuraciÃ³n 3S2P = 11.1V nominal
- BMS: 3S 20A para Li-ion 18650
- RegulaciÃ³n: 2x Buck Converter (uno para motores a 5V, uno para ESP32+sensores a 5V)
- Interruptor: Power switch
- Cables y conectores
```

#### Software

```
Framework: Arduino / PlatformIO
Bootloader: Espressif IDF

LibrerÃ­as:
- ArduinoJson: 6.21.0+
- VL53L0X: 1.3.0+ (sensores ToF cliff)
- ESP32-BLE-Arduino: Incluido en core
```

### 10.4 Red y Conectividad

```
Router WiFi:
- Frecuencia: 2.4GHz (ESP32 compatible)
- Seguridad: WPA2/WPA3
- DHCP: Habilitado con reservas IP

IP Addresses:
- Backend: IP fija 192.168.2.200
- Android: DHCP (conectado a misma red)
- ESP32: Solo Bluetooth (sin WiFi necesario)

Ancho de banda:
- Upload: 1 Mbps mÃ­nimo (para audio)
- Latencia: <100ms en LAN
```

---

## 11. Plan de Despliegue

### 11.0 Estrategia de ImplementaciÃ³n Incremental

La implementaciÃ³n se divide en tres componentes principales que se desarrollan y **validan de forma independiente** antes de pasar al siguiente. Esto permite detectar y corregir errores en cada capa sin depender de que todo estÃ© listo.

```
Orden de implementaciÃ³n:
  1. Backend Python/FastAPI (brinda la inteligencia del sistema)
  2. App Android (conecta al backend, aporta voz, cara y cÃ¡mara)
  3. ESP32 (aÃ±ade el cuerpo fÃ­sico al sistema)

Principio: Cada fase debe ser completamente funcional y testeada
           antes de comenzar la siguiente. Nunca integrar sin validar.
```

#### Herramientas de Prueba por Fase

```
Fase 1 (Backend solo):
  - Cliente WebSocket de escritorio: wscat, Postman, o scripts Python
  - Script de prueba de audio: enviar archivo .wav/-aac por WS y verificar respuesta en texto
  - Prueba de reconocimiento facial: REST API con imÃ¡genes JPEG
  - curl / HTTPie para endpoints REST (health, users, memory)

Fase 2 (Android + Backend, sin ESP32):
  - Robot virtual: App Android conectada al backend real
  - Verificar wake word, flujo facial, TTS y emojis en un telÃ©fono real
  - No se necesita ESP32 para validar la mayor parte de la experiencia
  - Simular comandos de movimiento y verificar que la app los envÃ­a pero no recibe confirmaciÃ³n ESP32

Fase 3 (Full stack):
  - ESP32 conectado vÃ­a BLE
  - Flujo completo: voz â†’ identificaciÃ³n â†’ respuesta hablada â†’ movimiento
```

### 11.1 Fase 1: ConfiguraciÃ³n del Backend

```
1. Preparar servidor (IP fija 192.168.2.200):
   â–¡ Instalar Ubuntu/preparar equipo
   â–¡ Actualizar sistema operativo
   â–¡ Instalar Docker + Docker Compose
   â–¡ Configurar IP estÃ¡tica 192.168.2.200
   
2. Clonar y configurar:
   â–¡ Clonar repositorio
   â–¡ Copiar .env.example a .env
   â–¡ Agregar GEMINI_API_KEY (Google AI Studio)
   â–¡ Generar API_KEY para Android
   â–¡ Crear directorio data/ y nginx/certs/
   
3. Configurar TLS para Nginx (OBLIGATORIO):
   â–¡ Generar certificado autofirmado:
     openssl req -x509 -newkey rsa:4096 -nodes \
       -keyout nginx/certs/key.pem \
       -out nginx/certs/cert.pem \
       -days 365 -subj "/CN=192.168.2.200"
   â–¡ Obtener fingerprint SHA-256 para certificate pinning:
     openssl x509 -in nginx/certs/cert.pem -pubkey -noout \
       | openssl pkey -pubin -outform der \
       | openssl dgst -sha256 -binary | base64
   â–¡ Agregar fingerprint a config.kt de Android
   
4. Desplegar con Docker Compose:
   â–¡ docker compose up -d
   â–¡ Verificar contenedores activos:
     docker compose ps
   â–¡ Revisar logs:
     docker compose logs -f
   â–¡ Acceder a https://192.168.2.200:9393/docs (acepta cert autofirmado)
   
5. Para actualizaciones:
   â–¡ docker compose pull (si hay nueva imagen)
   â–¡ docker compose up -d --build
   â–¡ docker compose logs -f fastapi
```

#### docker-compose.yml (referencia)

```yaml
version: "3.9"
services:
  fastapi:
    build: .
    container_name: robi_backend
    env_file: .env
    volumes:
      - ./data:/app/data
    expose:
      - "8000"
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: robi_nginx
    ports:
      - "9393:9393"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
    depends_on:
      - fastapi
    restart: unless-stopped
```

#### âœ… CÃ³mo probar el Backend sin Android ni ESP32

```
OpciÃ³n A â€” Streamlit Simulator (recomendada):
  1. Instalar Streamlit en el entorno de desarrollo:
     pip install streamlit websockets
  2. Ejecutar el simulador:
     streamlit run tests/streamlit_simulator/app.py
  3. El simulador provee una UI web que permite:
     a. Conectarse al backend vÃ­a WebSocket (wss://192.168.2.200:9393)
     b. Enviar mensajes de texto (simula el flujo de audio procesado)
     c. Simular inicio de interacciÃ³n con un usuario de prueba
     d. Visualizar en tiempo real: emotion tags, text_chunks, stream_end
     e. Ver el emoji correspondiente a la emociÃ³n recibida
     f. Simular comandos de movimiento y ver la respuesta move_sequence
     g. Revisar el historial de la sesiÃ³n actual
  4. Criterio de Ã©xito: El simulador muestra respuestas coherentes
     con emotion tag correcto â†’ backend funcional

OpciÃ³n B â€” WebSocket bÃ¡sico (wscat o script Python):
  1. Conectar al endpoint wss://192.168.2.200:9393/ws/interact
  2. Enviar mensaje JSON: {"type":"auth","api_key":"...","device_id":"test"}
  3. Verificar respuesta {"type":"auth_ok","session_id":"..."}

Prueba de audio real:
  Script Python:
    a. Conectar WebSocket
    b. Enviar interaction_start (user_id=unknown)
    c. Leer archivo .wav/.aac y enviarlo como binary chunks
    d. Enviar audio_end
    e. Verificar: emotion â†’ text_chunk(s) â†’ stream_end

Prueba de salud:
  curl https://192.168.2.200:9393/api/health
  Verificar: {"status":"ok","version":"1.4"}

Prueba 5 â€” REST auxiliar adicional (sin registro facial â€” el backend no gestiona caras):
  curl https://192.168.2.200:9393/api/health
  curl https://192.168.2.200:9393/api/users

Criterio de Ã©xito: El backend responde correctamente a todas las pruebas
anteriores â†’ se puede iniciar el desarrollo de la app Android.
```

### 11.2 Fase 2: Desarrollo App Android

```
1. Setup proyecto:
   â–¡ Android Studio instalado
   â–¡ Clonar repositorio
   â–¡ Sync Gradle dependencies
   â–¡ Configurar Android SDK
   
2. ConfiguraciÃ³n inicial:
   â–¡ Editar config.kt con URL del backend (https://)
   â–¡ Agregar API_KEY en EncryptedSharedPreferences
   â–¡ Configurar certificate pinning (fingerprint del servidor)
   â–¡ Configurar network_security_config.xml
   â–¡ Configurar wake word (descargar .ppn)
   
3. Build y test:
   â–¡ Build APK debug
   â–¡ Instalar en dispositivo
   â–¡ Otorgar permisos
   â–¡ Probar wake word detection
   â–¡ Probar conexiÃ³n WebSocket (wss://)
   â–¡ Verificar que text_chunks llegan y Android TTS los reproduce
   â–¡ Verificar emotion tags actualizan la cara del robot
   â–¡ Probar captura de foto ("Hey Robi, toma una foto")
   â–¡ Probar captura de video ("Hey Robi, graba un video de cinco segundos")
   â–¡ Verificar ServiceWatchdog funciona
   â–¡ Verificar configuraciÃ³n de voz TTS (velocidad, tono)
   
4. Pre-carga de emojis:
   â–¡ Ejecutar cache de emojis comunes
   â–¡ Verificar carga de OpenMoji
```

#### âœ… CÃ³mo probar la App Android sin ESP32

```
Prueba 1 â€” Flujo completo de interacciÃ³n (sin hardware fÃ­sico):
  1. Tener el backend corriendo y la app conectada por WiFi
  2. Decir "Hey Robi" â†’ verificar que la cara cambia a ğŸ‘‚ inmediatamente
  3. Hacer una pregunta â†’ verificar:
     a. Cara cambia a ğŸ¤” al enviar al backend
     b. Emotion tag llega antes del texto â†’ cara del robot actualiza
     c. Android TTS reproduce la respuesta fluidamente
     d. Secuencia de emojis contextuales se muestra
     e. Cara vuelve a ğŸ¤– (IDLE) al terminar

Prueba 2 â€” Reconocimiento facial on-device:
  1. Registrar un usuario vÃ­a REST (o la propia app)
  2. Decir "Hey Robi" y ponerse frente a la cÃ¡mara
  3. Verificar que reconoce el rostro (âœ… score > 0.70)
  4. Verificar saludo personalizado por TTS

Prueba 3 â€” Nuevo usuario:
  1. No registrar ningÃºn rostro
  2. Decir "Hey Robi" y mostrar la cara ante la cÃ¡mara
  3. Verificar que pregunta el nombre por TTS
  4. Responder con el nombre en voz alta
  5. Verificar que guarda el usuario y saluda con el nombre

Prueba 4 â€” Captura de foto:
  1. Decir "Hey Robi, toma una foto y dÃ­me quÃ© ves"
  2. Verificar que la cÃ¡mara activa y captura la foto
  3. Verificar que Gemini describe el contenido de la foto por TTS

Prueba 5 â€” DesconexiÃ³n y reconexion:
  1. Detener el backend mientras la app estÃ¡ activa
  2. Verificar que la cara muestra ğŸ”Œ y hay banner "Backend no disponible"
  3. Reiniciar el backend
  4. Verificar reconexion automÃ¡tica y vuelta a ğŸ¤–

Nota: Los comandos de movimiento se pueden simular; sin ESP32 simplemente
no habrÃ¡ confirmaciÃ³n de ejecuciÃ³n pero el resto del flujo funciona.

Criterio de Ã©xito: Toda la experiencia de interacciÃ³n de voz, reconocimiento
facial, expresiones visuales y captura de medios funciona sin el ESP32.
â†’ Ahora se puede iniciar la programaciÃ³n del ESP32.
```

### 11.3 Fase 3: ProgramaciÃ³n ESP32

```
1. Setup entorno:
   â–¡ Instalar PlatformIO
   â–¡ Conectar ESP32 por USB
   â–¡ Verificar puerto serial
   
2. ConfiguraciÃ³n:
   â–¡ Editar config.h (pines, constantes)
   â–¡ Compilar firmware
   â–¡ Flashear ESP32
   
3. Ensamblaje hardware:
   â–¡ Conectar L298N al ESP32 (pines EN/IN segÃºn config.h)
   â–¡ Conectar 2x Gear Motor TT Yellow al L298N
   â–¡ Verificar rueda de apoyo correctamente ensamblada
   â–¡ Conectar 3x VL53L0X ToF (cliff) por IÂ²C con XSHUT individuales
   â–¡ Conectar HC-SR04 FRONTAL (Trigger: GPIO 5, Echo: GPIO 18)
   â–¡ Conectar HC-SR04 TRASERO (Trigger: GPIO 19, Echo: GPIO 21)
   â–¡ Conectar RGB LED 4 patas (R: GPIO 23, G: GPIO 22, B: GPIO 4)
   â–¡ Conectar BMS 3S al pack de 6x 18650
   â–¡ Conectar Buck Converter #1 (motores): entrada BMS â†’ 5V salida â†’ L298N
   â–¡ Conectar Buck Converter #2 (ESP32): entrada BMS â†’ 5V salida â†’ VIN ESP32
   â–¡ Verificar voltajes con multÃ­metro:
       Pack cargado: ~12.6V, Buck #1 salida: 5.0V, Buck #2 salida: 5.0V
   â–¡ Conectar power switch
   
4. CalibraciÃ³n:
   â–¡ Probar motores individualmente (forward, backward, left, right)
   â–¡ Calibrar VL53L0X cliff (distancia de vaciÃ³ vs suelo)
   â–¡ Probar HC-SR04 frontal: colocar obstÃ¡culo a 9cm â†’ debe STOP
   â–¡ Probar HC-SR04 trasero: al retroceder con obstÃ¡culo a 9cm â†’ STOP
   â–¡ Probar RGB LED ciclo de colores (R, G, B, blanco)
   â–¡ Probar move_sequence con 2 steps
   
5. Test Bluetooth:
   â–¡ Emparejar con Android
   â–¡ Enviar comandos de prueba
   â–¡ Verificar telemetrÃ­a
   â–¡ Verificar heartbeat (matar app Android y confirmar BRAIN_OFFLINE)
   â–¡ Verificar LEDs Ã¡mbar pulsante en BRAIN_OFFLINE
   â–¡ Verificar recuperaciÃ³n al restaurar heartbeat
```

#### âœ… CÃ³mo probar el ESP32 sin la App Android completa

```
Prueba 1 â€” Scanner BLE + comandos directos:
  Usar app "nRF Connect" o "BLE Scanner" en el telÃ©fono para
  enviar JSON directamente al characteristic TX del ESP32.
  
  Comandos de prueba:
  {"type":"heartbeat","timestamp":12345}
  {"type":"move","direction":"forward","speed":30,"duration":2000}
  {"type":"light","action":"on","color":"green","intensity":80}
  {"type":"telemetry","request":"sensors"}

Prueba 2 â€” Timeout de heartbeat:
  1. Conectar con nRF Connect
  2. Enviar algunos heartbeats manualmente
  3. Dejar de enviar por 4 segundos
  4. Verificar que los motores se paran y LEDs cambian a Ã¡mbar pulsante
  5. Reanudar heartbeats y verificar recuperaciÃ³n

Prueba 3 â€” Sensores de seguridad:
  1. Activar movimiento hacia adelante
  2. Poner un obstÃ¡culo frente al sensor HC-SR04
  3. Verificar STOP automÃ¡tico

Criterio de Ã©xito: ESP32 responde a todos los comandos BLE y ejecuta
correctamente los protocolos de seguridad.
â†’ El sistema estÃ¡ listo para integraciÃ³n completa.
```

### 11.4 Fase 4: IntegraciÃ³n y Testing

```
1. Test IntegraciÃ³n Android-Backend:
   â–¡ Wake word â†’ grabaciÃ³n â†’ envÃ­o â†’ respuesta
   â–¡ Captura de imagen â†’ reconocimiento facial
   â–¡ VisualizaciÃ³n de emojis
   â–¡ ReproducciÃ³n de audio
   
2. Test IntegraciÃ³n Android-ESP32:
   â–¡ ConexiÃ³n Bluetooth estable
   â–¡ EnvÃ­o de comandos movimiento
   â–¡ RecepciÃ³n de telemetrÃ­a
   â–¡ ReconexiÃ³n automÃ¡tica
   
3. Test Sistema Completo:
   â–¡ Flujo: "Hey Robi" â†’ bÃºsqueda facial â†’ saludo â†’ comando â†’ respuesta â†’ acciÃ³n fÃ­sica
   â–¡ Escenario: Reconocer persona â†’ respuesta personalizada
   â–¡ Escenario: Comando movimiento â†’ ejecuciÃ³n + telemetrÃ­a
   â–¡ Escenario: DetecciÃ³n emergencia â†’ stop inmediato
   
4. Test de EstrÃ©s:
   â–¡ 50 interacciones consecutivas
   â–¡ MÃºltiples reconexiones Bluetooth
   â–¡ Backend con carga (10 req/min)
   â–¡ BaterÃ­a baja â†’ advertencias
```

### 11.5 Fase 5: Despliegue en ProducciÃ³n

```
1. Backend:
   â–¡ Configurar systemd service para auto-inicio
   â–¡ Configurar logrotate
   â–¡ Setup backup automÃ¡tico de DB
   â–¡ Configurar TLS con certificado autofirmado
   â–¡ Documentar IP, credenciales y fingerprint del cert
   â–¡ Configurar renovaciÃ³n de certificado (cron anual)
   
2. Android:
   â–¡ Build APK release (firmado)
   â–¡ Configurar certificate pinning con fingerprint del servidor
   â–¡ Instalar en dispositivo final
   â–¡ Configurar inicio automÃ¡tico de servicio
   â–¡ Verificar ServiceWatchdog activo
   â–¡ Configurar keep-alive
   
3. ESP32:
   â–¡ Ensamblar en carcasa final
   â–¡ Fijar componentes con pegamento/tornillos
   â–¡ Etiquetar switch de encendido
   â–¡ Documentar PIN Bluetooth
   
4. DocumentaciÃ³n usuario:
   â–¡ Manual de uso
   â–¡ Comandos de voz soportados
   â–¡ SoluciÃ³n de problemas comunes
   â–¡ Contacto soporte (tÃº)
```

---

## 12. MÃ©tricas y Monitoreo

### 12.1 KPIs del Sistema

```
Performance:
- Latencia wake word â†’ cambio visual: <100ms (instantÃ¡neo)
- Latencia wake word â†’ inicio grabaciÃ³n: <200ms
- Latencia primer text_chunk desde Gemini: <1s
- Latencia emotion tag â†’ cliente: <500ms (antes del texto)
- Android TTS: inicio reproducciÃ³n <200ms despuÃ©s del primer chunk
- Latencia comando BLE â†’ ejecuciÃ³n: <100ms
- Heartbeat roundtrip: <100ms

Fiabilidad:
- Uptime backend: >99%
- Tasa Ã©xito reconocimiento facial: >90%
- Tasa reconexiÃ³n BLE automÃ¡tica: >95%
- Tasa reconexiÃ³n WebSocket automÃ¡tica: >95%
- Heartbeat BRAIN_OFFLINE detecciÃ³n: <3s (100%)
- ServiceWatchdog recuperaciÃ³n: <60s
- DetecciÃ³n emergencias: 100%
- Coherencia emociÃ³n-respuesta: >95% (vs <60% con reglas)

Calidad:
- Naturalidad Android TTS (MOS): >3.5/5 (depende del motor del dispositivo)
- SatisfacciÃ³n usuario: Subjetiva

Recursos:
- Uso RAM backend: <2GB
- Uso CPU backend: <50% promedio
- Uso baterÃ­a Android: <5% por hora (servicio)
- AutonomÃ­a robot: >3 horas
```

### 12.2 Dashboard de Monitoreo (Opcional)

```
MÃ©tricas a visualizar:
- NÃºmero de interacciones por dÃ­a
- DistribuciÃ³n de tipos de comandos
- Tiempos de respuesta (percentiles)
- Errores por tipo
- Uso de memoria/CPU
- BaterÃ­a robot (histÃ³rico)

Herramientas sugeridas:
- Prometheus + Grafana (avanzado)
- Simple dashboard web en FastAPI
- Logs + grep (bÃ¡sico)
```

### 12.3 Alertas

```
CrÃ­ticas (requieren acciÃ³n inmediata):
- Backend down por >5 minutos (WebSocket caÃ­do)
- ESP32 en BRAIN_OFFLINE por >5 minutos
- ESP32 desconectado por >10 minutos
- BaterÃ­a <5%
- Error crÃ­tico en logs
- Certificado TLS prÃ³ximo a expirar (<30 dÃ­as)

Advertencias (revisar cuando sea posible):
- Tasa de errores >10% en Ãºltima hora
- Latencia primer text_chunk >1.5s
- ServiceWatchdog reiniciÃ³ servicio
- BaterÃ­a robot <15% (indicador rojo parpadeante en Android)
- Uso disco >80%

Informativas:
- Nuevo usuario registrado
- 100 interacciones completadas
- ActualizaciÃ³n de firmware disponible
```

---

## ApÃ©ndices

### A. Glosario de TÃ©rminos

```
Wake Word: Palabra clave para activar el robot ("Hey Robi")
STT: Speech-to-Text, conversiÃ³n de voz a texto
TTS: Text-to-Speech, conversiÃ³n de texto a voz
LLM: Large Language Model, modelo de lenguaje grande
Embedding: RepresentaciÃ³n vectorial numÃ©rica de datos
BLE: Bluetooth Low Energy
MTU: Maximum Transmission Unit
PWM: Pulse Width Modulation
ADC: Analog-to-Digital Converter
UUID: Universally Unique Identifier
MOS: Mean Opinion Score (mÃ©trica de calidad de voz sintetizada)
```

### B. Referencias y Recursos

```
DocumentaciÃ³n TÃ©cnica:
- FastAPI: https://fastapi.tiangolo.com
- LangChain Deep Agents: https://docs.langchain.com/oss/python/deepagents/overview
- LangGraph: https://docs.langchain.com/oss/python/langgraph/overview
- Google Gemini API: https://ai.google.dev/gemini-api/docs
- Google AI Studio: https://aistudio.google.com
- Porcupine: https://picovoice.ai/docs/porcupine
- OpenMoji: https://openmoji.org
- ESP32 Arduino: https://docs.espressif.com
- Android TextToSpeech: https://developer.android.com/reference/android/speech/tts/TextToSpeech

APIs y Servicios:
- Gemini Flash Lite: https://ai.google.dev/gemini-api/docs/models
- Google AI Studio (API Key): https://aistudio.google.com/apikey

LibrerÃ­as:
- deepagents: https://pypi.org/project/deepagents
- langchain-google-genai: https://pypi.org/project/langchain-google-genai
- google-generativeai (Python): https://github.com/google-gemini/generative-ai-python
- VL53L0X Arduino: https://github.com/pololu/vl53l0x-arduino
- Retrofit: https://square.github.io/retrofit
```

### C. Checklist de Desarrollo

```
Backend:
â–¡ WebSocket handler implementado y documentado
â–¡ Endpoints REST auxiliares implementados
â–¡ TLS via Nginx (Docker Compose) â€” Nginx maneja certs, FastAPI solo HTTP interno
â–¡ IntegraciÃ³n con Gemini Flash Lite (audio multimodal)
â–¡ LangChain Deep Agent (services/agent.py) con runtime LangGraph
â–¡ Agent sin tools (v1.4 base) â€” lista de tools=[] verificada
â–¡ Streaming de text_chunks al cliente (sin TTS en backend)
â–¡ Sistema de capture_request para foto/video
â–¡ Gemini con emotion tags en system prompt (TTS-safe)
â–¡ Parser de emotion tags implementado
â–¡ move_sequence en response_meta (total_duration_ms + steps)
â–¡ services/history.py â€” compactaciÃ³n del historial (cada 20 msgs)
â–¡ services/movement.py â€” cÃ¡lculo de move_sequence y total_duration_ms
â–¡ Filtro de privacidad en repositories/memory.py
â–¡ docker-compose.yml funcional (fastapi + nginx)
â–¡ Servidor en IP 192.168.2.200:9393 (Nginx TLS)
â–¡ Streamlit simulator funcional (tests/streamlit_simulator/app.py)
â–¡ Manejo de errores completo (incluyendo Gemini rate limit)
â–¡ Tests unitarios >70% cobertura
â–¡ Logging estructurado (structlog)
â–¡ ConfiguraciÃ³n por .env (incl. GEMINI_API_KEY, CONVERSATION_KEEP_ALIVE_MS)
â–¡ README con instrucciones setup (Docker Compose + TLS)
â–¡ Parser de emotion tags implementado
â–¡ Manejo de errores completo (incluyendo Gemini rate limit)
â–¡ Tests unitarios >70% cobertura
â–¡ Logging estructurado (structlog)
â–¡ ConfiguraciÃ³n por .env (incl. GEMINI_API_KEY)
â–¡ README con instrucciones setup (incl. TLS)
â–¡ Script de prueba WS sin Android (wscat/Python)

Android:
â–¡ Arquitectura MVVM implementada
â–¡ Modo landscape fijo (no orientaciÃ³n automÃ¡tica)
â–¡ Tema oscuro â€” fondo negro (#000000), texto azul metÃ¡lico (#88CCEE)
â–¡ Sin botones â€” control solo por voz
â–¡ Emoji OpenMoji 80% pantalla, texto 10% debajo â€” descarga CDN automÃ¡tica
â–¡ BaterÃ­a robot â‰¤15% â†’ indicador rojo parpadeante (esquina superior izquierda)
â–¡ BaterÃ­a telÃ©fono â†’ indicador naranja parpadeante (esquina superior derecha)
â–¡ WebSocket client: envÃ­o audio, recepciÃ³n text_chunks
â–¡ Certificate pinning configurado (192.168.2.200:9393)
â–¡ ServiceWatchdog (AlarmManager) activo
â–¡ HeartbeatSender (BLE cada 1s) implementado
â–¡ Emotion tag parser para expresiones
â–¡ TtsManager: Android TextToSpeech configurado (velocidad, tono, idioma)
â–¡ ReproducciÃ³n TTS en streaming (chunk por chunk, a nivel de oraciÃ³n)
â–¡ PhotoVideoCaptureService: foto y video por comando de voz (CÃMARA FRONTAL)
â–¡ TransiciÃ³n visual instantÃ¡nea al wake word
â–¡ Modo escucha continua (60s) â€” CONVERSATION_KEEP_ALIVE_MS
â–¡ Flujo de bÃºsqueda de persona: ESP32 search_rotate Â±90Â° + timeout 8s
â–¡ Reconocimiento facial SOLO en cÃ¡mara frontal (ML Kit)
â–¡ send_move_sequence a ESP32 cuando backend devuelve steps
â–¡ Manejo de permisos runtime (incl. CAMERA, RECORD_AUDIO)
â–¡ ReconexiÃ³n automÃ¡tica WebSocket + BLE
â–¡ Cache de 20 emojis OpenMoji comunes (resto descarga CDN on-demand)
â–¡ EncryptedSharedPreferences (incl. parÃ¡metros TTS)
â–¡ Logs de debugging
â–¡ APK release firmado

ESP32:
â–¡ Todos los sensores funcionando:
  â–¡ HC-SR04 FRONTAL (detenciÃ³n < 10cm al avanzar)
  â–¡ HC-SR04 TRASERO (detenciÃ³n < 10cm al retroceder)
  â–¡ 3x VL53L0X ToF cliff (detecciÃ³n por IÂ²C con XSHUT individuales)
â–¡ RGB LED 4 patas funcionando (modos: solid, blink, breathe)
â–¡ L298N + Gear Motor TT Yellow 5V â€” 2 ruedas + soporte
â–¡ move_sequence ejecutado en orden con duraciones correctas
â–¡ search_rotate (Â±90Â°) para bÃºsqueda de persona
â–¡ BaterÃ­a 3S2P correctamente leÃ­da (ADC GPIO 35): rango 9.0â€“12.6V
â–¡ HeartbeatMonitor implementado (timeout 3s)
â–¡ Estado BRAIN_OFFLINE con LEDs Ã¡mbar
â–¡ Sistema de seguridad activo (cliff + heartbeat + distancia)
â–¡ TelemetrÃ­a completa (incl. estado heartbeat, ambos sensores distancia)
â–¡ ReconexiÃ³n BLE automÃ¡tica
â–¡ LEDs indicadores de estado (incl. BRAIN_OFFLINE = Ã¡mbar)
â–¡ CÃ³digo comentado
â–¡ Probado con nRF Connect antes de integrar con Android
```

---

## ConclusiÃ³n

Este documento define la arquitectura completa del sistema robÃ³tico domÃ©stico interactivo. La arquitectura estÃ¡ diseÃ±ada para:

âœ… **InteracciÃ³n Fluida**: WebSocket streaming entrega texto en tiempo real, Android TTS habla sin latencia de red  
âœ… **LLM Multimodal**: Gemini Flash Lite procesa audio, imagen y video directamente (sin STT separado)  
âœ… **Agente Extensible**: LangChain Deep Agents como harness con runtime LangGraph; preparado para tools, Skills y MCP futuros  
âœ… **Expresividad Coherente**: Emociones dirigidas por el LLM, sincronizadas con la intenciÃ³n de la respuesta  
âœ… **VisiÃ³n Activa**: La cÃ¡mara FRONTAL responde a comandos de voz para capturar fotos y videos  
âœ… **Seguridad FÃ­sica**: Heartbeat en ESP32 protege al robot si el cerebro Android falla; 2 sensores HC-SR04 (frontal + trasero) detectan obstÃ¡culos a < 10cm  
âœ… **Seguridad de Datos**: HTTPS/WSS via Nginx + certificate pinning, red local tratada como territorio hostil  
âœ… **Robustez**: ServiceWatchdog, reconexiÃ³n automÃ¡tica, manejo exhaustivo de errores  
âœ… **Bajo Costo**: Gemini Flash Lite (muy econÃ³mico), TTS nativo Android (gratis), `deepagents` (open source), OpenMoji CDN (gratis)  
âœ… **Despliegue Simple**: Docker Compose (FastAPI + Nginx) â€” un solo `docker compose up -d`  
âœ… **Privacidad**: Reconocimiento facial 100% on-device (ML Kit); el backend nunca procesa rostros  
âœ… **UI Expresiva**: Landscape fija, tema oscuro, emoji 80% pantalla, sin botones, control por voz  
âœ… **ImplementaciÃ³n Incremental**: Cada componente se valida de forma independiente antes de integrar; Streamlit simulator para pruebas sin hardware  
âœ… **Mantenibilidad**: CÃ³digo estructurado, bien documentado  

La implementaciÃ³n debe seguir este documento como guÃ­a, ajustando detalles segÃºn necesidades especÃ­ficas durante el desarrollo. Cada componente (Backend, Android, ESP32) puede ser desarrollado independientemente y luego integrado siguiendo los protocolos definidos.

---

**AprobaciÃ³n y Firmas**

| Rol | Nombre | Fecha | Firma |
|-----|--------|-------|-------|
| Arquitecto de Sistema | [A completar] | | |
| Lead Backend | [A completar] | | |
| Lead Android | [A completar] | | |
| Lead Embedded | [A completar] | | |

---

**Control de Versiones**

| VersiÃ³n | Fecha | Autor | Cambios |
|---------|-------|-------|---------|
| 1.0 | 2026-02-08 | Claude | Documento inicial completo |
| 1.1 | 2026-02-08 | Claude | RevisiÃ³n post-evaluaciÃ³n: arquitectura WebSocket streaming, heartbeat ESP32, emociones dirigidas por LLM, HTTPS obligatorio con certificate pinning |
| 1.2 | 2026-02-08 | Claude | Flujo de activaciÃ³n y reconocimiento facial on-device (ML Kit + FaceNet TFLite) |
| 1.3 | 2026-02-18 | Claude | Ajustes: TTS Android nativo (reemplaza Piper/ElevenLabs), LLM migrado a Gemini Flash Lite, LangChain Deep Agents como framework del agente (extensible con MCP/tools/skills), captura de foto/video por comando de voz, system prompt TTS-safe, plan de implementaciÃ³n incremental con pruebas por fase |
| 1.4 | 2026-02-18 | Claude | Ajustes 1-24: bÃºsqueda persona con rotaciÃ³n Â±90Â° (PERSON_SEARCH_TIMEOUT_MS=8s), solo cÃ¡mara frontal, escucha continua 60s (CONVERSATION_KEEP_ALIVE_MS), landscape fija + tema oscuro + emoji OpenMoji CDN, control solo por voz, historial con compactaciÃ³n (20 msgs) + filtro privacidad, indicadores baterÃ­a â‰¤15%, secuencias de movimiento ESP32 + total_duration_ms, Docker Compose (Nginx+FastAPI), eliminado reconocimiento facial backend, 2 ruedas + apoyo, 2 sensores distancia HC-SR04, RGB LED 4 patas, solo L298N + Gear Motor TT Yellow 5V, VL53L0X ToF cliff, pack 6x18650 3S2P + BMS 3S 20A + 2 buck converters, IP 192.168.2.200:9393, Streamlit simulator, OpenMoji sin ZIP |
